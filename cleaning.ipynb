{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1761597963568
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polars version: 1.34.0\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set Polars to use a larger string cache, which can be beneficial\n",
        "# for high-cardinality string columns found in large datasets.\n",
        "pl.enable_string_cache()\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1761597966583
        }
      },
      "outputs": [],
      "source": [
        "def analyze_and_clean_imdb(url: str, output_filename: str = \"imdb_cleaned_ml_ready.parquet\"):\n",
        "    \"\"\"\n",
        "    Loads, analyzes, and cleans the large IMDB Parquet dataset efficiently\n",
        "    using Polars' lazy API.\n",
        "\n",
        "    - We fill missing numeric values with -1 (temporary)\n",
        "    - We keep list columns as lists\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Efficient Loading (Lazy Scan) ---\n",
        "        print(\"1. Creating LazyFrame (scanning schema)...\")\n",
        "        lf = pl.scan_parquet(url)\n",
        "        print(\"   ...Collecting schema.\")\n",
        "        all_columns = lf.collect_schema().names()\n",
        "        print(f\"   ...Schema loaded. Found {len(all_columns)} columns.\")\n",
        "\n",
        "        # --- Step 2: Automated Analysis ---\n",
        "        print(\"\\n2. Building analysis query (nulls, uniques, redundancy)...\")\n",
        "\n",
        "        def are_cols_identical(col_a: str, col_b: str) -> pl.Expr:\n",
        "            \"\"\"Helper expression for null-safe equality check.\"\"\"\n",
        "            return (\n",
        "                (pl.col(col_a) == pl.col(col_b)).fill_null(False) |\n",
        "                (pl.col(col_a).is_null() & pl.col(col_b).is_null())\n",
        "            ).all()\n",
        "\n",
        "        # Build a single, parallel query to get all metadata at once\n",
        "        analysis_lf = lf.select(\n",
        "            pl.len().alias(\"total_rows\"),\n",
        "            *[pl.col(c).null_count().alias(f\"{c}_null_count\") for c in all_columns],\n",
        "            *[pl.col(c).n_unique().alias(f\"{c}_n_unique\") for c in all_columns]\n",
        "        )\n",
        "\n",
        "        print(\"   ...Executing analysis (1 full pass). This is the slowest step.\")\n",
        "        start_analysis_time = time.time()\n",
        "        # .collect(engine='streaming') runs the analysis in chunks\n",
        "        analysis_results_raw = analysis_lf.collect(engine='streaming')\n",
        "        analysis_dict = analysis_results_raw.to_dicts()[0]\n",
        "        print(f\"   ...Analysis complete in {time.time() - start_analysis_time:.2f} seconds.\\n\")\n",
        "\n",
        "        # --- Process Analysis Results ---\n",
        "        total_rows = analysis_dict[\"total_rows\"]\n",
        "        null_percentages = {}\n",
        "        unique_counts = {}\n",
        "\n",
        "        for col in all_columns:\n",
        "            null_count = analysis_dict.get(f\"{col}_null_count\", 0)\n",
        "            null_percentages[col] = (null_count / total_rows) * 100\n",
        "            unique_counts[col] = analysis_dict.get(f\"{col}_n_unique\", 0)\n",
        "\n",
        "\n",
        "        # --- Step 3: Build Lazy Cleaning Pipeline ---\n",
        "        print(\"\\n3. Building data cleaning and transformation pipeline...\")\n",
        "        lf_cleaned = lf.lazy()\n",
        "\n",
        "        # --- 3a. Column Dropping ---\n",
        "        cols_to_drop = set()\n",
        "        \n",
        "        # Drop columns with only one unique value (no variance)\n",
        "        low_variance_cols = {c for c, u in unique_counts.items() if u <= 1}\n",
        "        print(f\"Low variance (1 unique value) cols: {low_variance_cols or 'None'}\")\n",
        "        cols_to_drop.update(low_variance_cols)\n",
        "\n",
        "        # Drop columns that are almost entirely null\n",
        "        high_null_cols = {c for c, p in null_percentages.items() if p > 95.0}\n",
        "        high_null_cols.discard('endYear') # Specific to keep endYear\n",
        "        print(f\"High null (>95%) cols: {high_null_cols or 'None'}\")\n",
        "        cols_to_drop.update(high_null_cols)\n",
        "\n",
        "\n",
        "        # User-specified domain drops (e.g., foreign keys, IDs)\n",
        "        domain_drops = {\n",
        "            'tconst', 'directors', 'writers', 'titleId',\n",
        "            'tconst_1', 'nconst', 'nconst_1', 'parentTconst', 'knownForTitles'\n",
        "        }\n",
        "        print(f\"User-specified domain-based drops: {domain_drops}\")\n",
        "        cols_to_drop.update(domain_drops)\n",
        "        \n",
        "        final_cols_to_drop = list(cols_to_drop.intersection(set(all_columns)))\n",
        "        lf_cleaned = lf_cleaned.drop(final_cols_to_drop)\n",
        "        cleaned_columns = [c for c in all_columns if c not in final_cols_to_drop]\n",
        "        print(f\"   ...Dropping {len(final_cols_to_drop)} columns.\")\n",
        "\n",
        "        # --- 3b. Missing Value Imputation & Cleaning ---\n",
        "        print(\"   ...Applying user-specified imputations and cleaning.\")\n",
        "        cleaning_and_imputation_steps = []\n",
        "\n",
        "        # Numeric columns: fill missing with -1\n",
        "        numeric_cols = [\n",
        "            'startYear', 'runtimeMinutes', 'seasonNumber', 'episodeNumber',\n",
        "            'birthYear', 'deathYear'\n",
        "        ]\n",
        "        for col in numeric_cols:\n",
        "            if col in cleaned_columns:\n",
        "                cleaning_and_imputation_steps.append(pl.col(col).fill_null(-1))\n",
        "\n",
        "        if 'averageRating' in cleaned_columns:\n",
        "            cleaning_and_imputation_steps.append(pl.col('averageRating').fill_null(-1.0))\n",
        "        if 'numVotes' in cleaned_columns:\n",
        "            cleaning_and_imputation_steps.append(pl.col('numVotes').fill_null(-1))\n",
        "        if 'isAdult' in cleaned_columns:\n",
        "            cleaning_and_imputation_steps.append(pl.col('isAdult').fill_null(-1))\n",
        "\n",
        "        # List columns (keep as list, clean commas)\n",
        "        list_cols = ['genres', 'primaryProfession']\n",
        "        # This regex keeps letters, numbers, and commas, removing other junk.\n",
        "        list_junk_removal_regex = r\"[^\\p{L}\\p{N},]\"\n",
        "        for col in list_cols:\n",
        "            if col in cleaned_columns:\n",
        "                cleaning_and_imputation_steps.append(\n",
        "                    pl.col(col)\n",
        "                      .str.replace_all(list_junk_removal_regex, \"\")\n",
        "                      .str.split(',')\n",
        "                      .fill_null([]) # Fill missing with an empty list\n",
        "                )\n",
        "\n",
        "        # Characters column: extract all quoted strings into a list\n",
        "        if 'characters' in cleaned_columns:\n",
        "            cleaning_and_imputation_steps.append(\n",
        "                pl.col('characters')\n",
        "                  .str.extract_all(r'\"(.*?)\"')\n",
        "                  .fill_null([])\n",
        "            )\n",
        "\n",
        "        # General strings: strip whitespace, fill nulls, keep symbols\n",
        "        general_string_cols = [\n",
        "            'titleType', 'primaryTitle', 'title',\n",
        "            'region', 'language', 'types', 'category', 'job'\n",
        "        ]\n",
        "        for col in general_string_cols:\n",
        "            if col in cleaned_columns:\n",
        "                cleaning_and_imputation_steps.append(\n",
        "                    pl.col(col)\n",
        "                      .str.strip_chars()\n",
        "                      .fill_null('Unknown')\n",
        "                )\n",
        "\n",
        "        lf_cleaned = lf_cleaned.with_columns(cleaning_and_imputation_steps)\n",
        "\n",
        "        # --- 3c. Data Type Optimization ---\n",
        "        print(\"   ...Applying data type optimizations (casting).\")\n",
        "        type_casting_steps = []\n",
        "\n",
        "        if 'isAdult' in cleaned_columns:\n",
        "            type_casting_steps.append(pl.col('isAdult').cast(pl.UInt8))\n",
        "        if 'isOriginalTitle' in cleaned_columns:\n",
        "            type_casting_steps.append(pl.col('isOriginalTitle').cast(pl.Boolean))\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in cleaned_columns:\n",
        "                type_casting_steps.append(pl.col(col).cast(pl.Int32))\n",
        "\n",
        "        # Specific V14 logic for 'endYear'\n",
        "        if 'endYear' in cleaned_columns:\n",
        "            # Fill nulls with -1 AND cast to Int32.\n",
        "            # cast(strict=False) handles weird formats like \"2007.0\"\n",
        "            type_casting_steps.append(\n",
        "                pl.col('endYear').fill_null(-1).cast(pl.Int32, strict=False)\n",
        "            )\n",
        "            \n",
        "        if 'numVotes' in cleaned_columns:\n",
        "            type_casting_steps.append(pl.col('numVotes').cast(pl.Int64))\n",
        "        if 'averageRating' in cleaned_columns:\n",
        "            type_casting_steps.append(pl.col('averageRating').cast(pl.Float32))\n",
        "\n",
        "        # Convert moderate-cardinality strings to Categorical for memory savings\n",
        "        current_schema = lf_cleaned.collect_schema()\n",
        "        for col in cleaned_columns:\n",
        "            \n",
        "            # Check if it's a string AND not 'endYear' (which has special handling)\n",
        "            if col != 'endYear' and col in current_schema and current_schema[col] == pl.String:\n",
        "            \n",
        "                # Use pre-calculated unique counts to make decisions\n",
        "                if 1 < unique_counts.get(col, 10001) < 10000:\n",
        "                    print(f\"   ...Casting '{col}' to Categorical ({unique_counts[col]} unique).\")\n",
        "                    type_casting_steps.append(pl.col(col).cast(pl.Categorical))\n",
        "\n",
        "        lf_cleaned = lf_cleaned.with_columns(type_casting_steps)\n",
        "\n",
        "        # --- Print final schema before saving ---\n",
        "        print(\"\\nFinal schema that will be saved:\")\n",
        "        print(lf_cleaned.collect_schema())\n",
        "\n",
        "        # --- Step 4: Execute Pipeline and Sink to Parquet ---\n",
        "        print(f\"\\n4. Executing full pipeline and sinking to '{output_filename}' (streaming)...\")\n",
        "        start_sink_time = time.time()\n",
        "        \n",
        "        # This is the second and final full pass over the data.\n",
        "        # It reads, cleans, and writes all in one streaming operation.\n",
        "        lf_cleaned.sink_parquet(\n",
        "            output_filename,\n",
        "            compression=\"zstd\",\n",
        "            engine='streaming'\n",
        "        )\n",
        "        print(f\"   ...Streaming sink complete in {time.time() - start_sink_time:.2f} seconds.\")\n",
        "        print(\"\\n--- Pipeline Finished! ---\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An Error Occurred ---\", file=sys.stderr)\n",
        "        print(f\"Error: {e}\", file=sys.stderr)\n",
        "        print(\"Please check the URL and your network connection.\", file=sys.stderr)\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pipeline Summary from above\n",
        "\n",
        "### **Step 1: Lazy Scan (`pl.scan_parquet`)**\n",
        "- **Instantaneous.**  \n",
        "- Polars reads only the metadata (schema) of the Parquet file, not the data itself.  \n",
        "- This allows us to plan operations on massive files (e.g., 400M or 40B rows) **without loading any data into RAM**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Automated Analysis (`analysis_lf.collect`)**\n",
        "- **First full pass over the data.**  \n",
        "- Builds a **single, complex query** to compute:\n",
        "  - `total_rows`\n",
        "  - `null_count` for every column  \n",
        "  - `n_unique` for every column  \n",
        "- Using `engine='streaming'`, Polars runs this query in **small chunks**, aggregating results efficiently.  \n",
        "- The output is a small dictionary (`analysis_dict`) used to dynamically construct the cleaning plan.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Build Lazy Cleaning Pipeline**\n",
        "- **Instantaneous.** Only defines the plan — no data is processed yet.\n",
        "\n",
        "#### **3a. Column Dropping**\n",
        "Automatically removes columns based on:\n",
        "- **Low Variance:** Columns with only 1 unique value  \n",
        "- **High Nulls:** Columns with >95% missing values  \n",
        "\n",
        "#### **3b. Imputation & Cleaning**\n",
        "Defines business logic:\n",
        "- Filling nulls  \n",
        "- Splitting strings into lists  \n",
        "- Extracting regex patterns  \n",
        "\n",
        "#### **3c. Type Optimization**\n",
        "- Downcasts numeric types (e.g., `Int64 → Int32`, `UInt8`) to save memory.  \n",
        "- Converts string columns with medium cardinality (2–10k unique values) to **Categorical**, improving memory efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Execute and Sink (`lf_cleaned.sink_parquet`)**\n",
        "- **Second and final full pass over the data.**  \n",
        "- Executes the entire optimized plan (Steps 1, 3a, 3b, and 3c).  \n",
        "- With `engine='streaming'`, the **read → transform → write** process runs in chunks, keeping RAM usage minimal from start to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1761598999768
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Creating LazyFrame (scanning schema)...\n",
            "   ...Collecting schema.\n",
            "   ...Schema loaded. Found 36 columns.\n",
            "\n",
            "2. Building analysis query (nulls, uniques, redundancy)...\n",
            "   ...Executing analysis (1 full pass). This is the slowest step.\n",
            "   ...Analysis complete in 511.34 seconds.\n",
            "\n",
            "\n",
            "3. Building data cleaning and transformation pipeline...\n",
            "Low variance (1 unique value) cols: None\n",
            "High null (>95%) cols: {'attributes'}\n",
            "User-specified domain-based drops: {'tconst', 'nconst_1', 'knownForTitles', 'writers', 'directors', 'tconst_1', 'nconst', 'parentTconst', 'titleId'}\n",
            "   ...Dropping 10 columns.\n",
            "   ...Applying user-specified imputations and cleaning.\n",
            "   ...Applying data type optimizations (casting).\n",
            "   ...Casting 'titleType' to Categorical (11 unique).\n",
            "   ...Casting 'region' to Categorical (250 unique).\n",
            "   ...Casting 'language' to Categorical (111 unique).\n",
            "   ...Casting 'types' to Categorical (24 unique).\n",
            "   ...Casting 'category' to Categorical (14 unique).\n",
            "\n",
            "Final schema that will be saved:\n",
            "Schema([('titleType', Categorical), ('primaryTitle', String), ('originalTitle', String), ('isAdult', UInt8), ('startYear', Int32), ('endYear', Int32), ('runtimeMinutes', Int32), ('genres', List(String)), ('averageRating', Float32), ('numVotes', Int64), ('seasonNumber', Int32), ('episodeNumber', Int32), ('ordering', Int64), ('title', String), ('region', Categorical), ('language', Categorical), ('types', Categorical), ('isOriginalTitle', Boolean), ('ordering_1', Int64), ('category', Categorical), ('job', String), ('characters', List(String)), ('primaryName', String), ('birthYear', Int32), ('deathYear', Int32), ('primaryProfession', List(String))])\n",
            "\n",
            "4. Executing full pipeline and sinking to 'imdb_cleaned_full.parquet' (streaming)...\n",
            "   ...Streaming sink complete in 480.81 seconds.\n",
            "\n",
            "--- Pipeline Finished! ---\n",
            "\n",
            "Successfully created and saved the final, ML-ready DataFrame to 'imdb_cleaned_full.parquet'.\n",
            "Total pipeline time: 993.22 seconds.\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "load_dotenv(\"config/.env\")\n",
        "# This URL points to the dataset\n",
        "DATASET_URL = os.getenv(\"IMDB_RAW_MERGED_URL\")\n",
        "\n",
        "# The final, ML-ready file that will be created\n",
        "OUTPUT_FILE = \"imdb_cleaned_full.parquet\"\n",
        "\n",
        "# --- Run the Pipeline ---\n",
        "start_total_time = time.time()\n",
        "success = analyze_and_clean_imdb(DATASET_URL, OUTPUT_FILE)\n",
        "\n",
        "if success:\n",
        "    print(f\"\\nSuccessfully created and saved the final, ML-ready DataFrame to '{OUTPUT_FILE}'.\")\n",
        "    print(f\"Total pipeline time: {time.time() - start_total_time:.2f} seconds.\")\n",
        "else:\n",
        "    print(f\"\\nPipeline failed. Please see error messages above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1761597931170
        }
      },
      "outputs": [],
      "source": [
        "# This script generates .csv file out of the .parquet file for preview purposes.\n",
        "# Only top rows are taken, not a random sample.\n",
        "\n",
        "# --- Configuration ---\n",
        "n_rows = 5000  # Set the desired number of sample rows.\n",
        "OUTPUT_FILE = OUTPUT_FILE # Source Parquet file name.\n",
        "CSV_FILE = \"preview.csv\"             # Output CSV file name.\n",
        "\n",
        "print(f\"Taking first {n_rows} rows from '{OUTPUT_FILE}'...\")\n",
        "\n",
        "try:\n",
        "    # 1. Open the Parquet file\n",
        "    pq_file = pq.ParquetFile(OUTPUT_FILE)\n",
        "\n",
        "    # 2. Initialize variables\n",
        "    rows_collected = 0  # Counter for rows collected.\n",
        "    dfs = []              # List to store DataFrame batches.\n",
        "\n",
        "    # 3. Iterate over file in batches\n",
        "    # Read file in chunks for memory efficiency.\n",
        "    for batch in pq_file.iter_batches(batch_size=100_000):\n",
        "        \n",
        "        # Convert the raw batch into a PyArrow Table.\n",
        "        table = pa.Table.from_batches([batch])\n",
        "\n",
        "        # 4. Convert dictionary/categorical columns\n",
        "        # Cast dictionary types (Polars' 'Categorical') to strings for Pandas compatibility.\n",
        "        for col_name, col_type in zip(table.schema.names, table.schema.types):\n",
        "            if pa.types.is_dictionary(col_type):\n",
        "                # Cast the column to string.\n",
        "                table = table.set_column(\n",
        "                    table.schema.get_field_index(col_name),\n",
        "                    col_name,\n",
        "                    table[col_name].cast(pa.string())\n",
        "                )\n",
        "        \n",
        "        # Convert batch to Pandas DataFrame.\n",
        "        df = table.to_pandas()\n",
        "        \n",
        "        # 5. Slice and collect rows\n",
        "        \n",
        "        # Calculate rows still needed.\n",
        "        remaining = n_rows - rows_collected\n",
        "        \n",
        "        # Get the slice of rows needed from this batch.\n",
        "        df_slice = df.iloc[:remaining]\n",
        "        \n",
        "        # Add the slice to our list.\n",
        "        dfs.append(df_slice)\n",
        "        \n",
        "        # Update our counter.\n",
        "        rows_collected += len(df_slice)\n",
        "        \n",
        "        # 6. Check if done\n",
        "        # Exit loop if we have enough rows.\n",
        "        if rows_collected >= n_rows:\n",
        "            break\n",
        "\n",
        "    # 7. Combine and save to CSV\n",
        "    print(\"\\nConcatenating collected batches...\")\n",
        "    \n",
        "    # Concatenate all collected DataFrame slices.\n",
        "    sample_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Save the sample DataFrame to a CSV file.\n",
        "    # `index=False` avoids saving the Pandas index.\n",
        "    sample_df.to_csv(CSV_FILE, index=False)\n",
        "    \n",
        "    print(f\"Success! CSV '{CSV_FILE}' created with first {n_rows} rows.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
