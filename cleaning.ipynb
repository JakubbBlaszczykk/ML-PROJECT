{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ IMDb Movie Data Cleaning Notebook\n",
    "\n",
    "This notebook performs a comprehensive cleaning process on the `big_imdb_table_flat_akas_v6.parquet` dataset. The goal is to prepare the data for potential machine learning tasks by addressing missing values, inconsistencies, and potential errors, while keeping the data largely human-readable.\n",
    "\n",
    "**Approach:**\n",
    "* **Polars for Efficiency:** We use the Polars library for fast, memory-efficient data manipulation, especially for handling large datasets and complex nested structures.\n",
    "* **Step-by-Step Cleaning:** We apply cleaning steps sequentially, starting with broad column removals and moving towards finer-grained data sanitization.\n",
    "* **Data-Driven Decisions:** Many decisions (like dropping columns or mapping values) are based on initial data exploration (EDA) and insights gained during the process (e.g., identifying single-value columns, understanding the structure of `primaryProfession`).\n",
    "* **Sklearn Pipeline:** A final `sklearn` pipeline handles placeholder imputation for remaining top-level missing values, ensuring reproducibility for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars scikit-learn # Ensure libraries are installed\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import set_config\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "# Set sklearn to output Polars DataFrames\n",
    "try:\n",
    "    set_config(transform_output=\"polars\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not set sklearn config: {e}. Proceeding without it.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "\n",
    "# Define file path\n",
    "DATA_FILE = \"big_imdb_table_flat_akas_v6.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Load Data\n",
    "\n",
    "**What:** Load the Parquet file into a Polars DataFrame.\n",
    "**Why:** To begin the cleaning process.\n",
    "**How:** Using `pl.read_parquet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pl.read_parquet(DATA_FILE)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file '{DATA_FILE}': {e}\")\n",
    "    print(\"Exiting. Please check the file path.\")\n",
    "    # In a real script, you might raise the exception or exit\n",
    "    # For a notebook, we'll let it proceed but df might not be defined\n",
    "    raise e \n",
    "\n",
    "print(\"--- Original DataFrame Head ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Original Schema ---\")\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Drop Unnecessary Top-Level Columns\n",
    "\n",
    "**What:** Remove identifier columns (`tconst`, `ordering`) and redundant title columns (`primaryTitle`, `title`, `originalTitle`).\n",
    "**Why:** \n",
    "* Identifiers like `tconst` and `ordering` provide no generalizable information for modeling.\n",
    "* We decided based on prior inspection that `primaryTitle`, `title`, and `originalTitle` were either too similar or we only needed one canonical title (which we're removing here, implying title information might be used differently later or isn't needed for the target task). Keeping only one reduces redundancy.\n",
    "**How:** Using `df.drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 1: Dropping top-level ID and title columns ---\")\n",
    "cols_to_drop_step1 = ['tconst', 'ordering', 'primaryTitle', 'title', 'originalTitle'] \n",
    "existing_cols_to_drop = [col for col in cols_to_drop_step1 if col in df.columns]\n",
    "\n",
    "if existing_cols_to_drop:\n",
    "    print(f\"Dropping existing top-level columns: {existing_cols_to_drop}\")\n",
    "    df = df.drop(existing_cols_to_drop)\n",
    "else:\n",
    "    print(\"No specified columns ('tconst', 'ordering', 'primaryTitle', 'title', 'originalTitle') found to drop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Check for and Drop Duplicate Rows\n",
    "\n",
    "**What:** Identify and remove rows that are exact duplicates after dropping initial identifiers.\n",
    "**Why:** Duplicate rows can skew analysis and model training. It's standard practice to remove them.\n",
    "**How:** Using `df.is_duplicated()` to find and `df.unique()` to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 1.5: Checking for duplicate rows ---\")\n",
    "duplicates_df = df.filter(df.is_duplicated())\n",
    "\n",
    "if duplicates_df.height > 0:\n",
    "    print(f\"Found {duplicates_df.height} duplicate rows. Showing first 5:\")\n",
    "    # Display relevant columns to understand the duplicates\n",
    "    cols_to_show_duplicates = [col for col in ['titleType', 'startYear', 'runtimeMinutes', 'genres'] if col in duplicates_df.columns]\n",
    "    print(duplicates_df.head(5).select(cols_to_show_duplicates))\n",
    "    print(\"Dropping duplicate rows...\")\n",
    "    df = df.unique(keep='first')\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Drop Top-Level Single-Value Columns\n",
    "\n",
    "**What:** Remove columns where all rows contain the same single value (or are all null).\n",
    "**Why:** Such columns have zero variance and provide no predictive information for a model.\n",
    "**How:** Calculate `n_unique()` for each column and drop those where the count is <= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 2: Dropping top-level single-value columns ---\")\n",
    "cardinality = df.select(pl.all().n_unique())\n",
    "cols_to_drop_step2 = [\n",
    "    col.name for col in cardinality.select(pl.all())\n",
    "    if col[0] is not None and col[0] <= 1\n",
    "]\n",
    "if cols_to_drop_step2:\n",
    "    print(f\"Dropping single-value columns: {cols_to_drop_step2}\")\n",
    "    df = df.drop(cols_to_drop_step2)\n",
    "else:\n",
    "    print(\"No single-value columns to drop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Drop Top-Level High-Null Columns\n",
    "\n",
    "**What:** Remove columns where more than 90% of the values are null.\n",
    "**Why:** These columns contain very little information and attempts to impute them often introduce more noise than signal.\n",
    "**How:** Calculate `null_count()` as a percentage of total rows and drop columns exceeding the 90% threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 3: Dropping >90% null top-level columns ---\") # Renumbered from 4\n",
    "null_percentages = df.null_count() / len(df)\n",
    "cols_to_drop_step3 = [\n",
    "    col.name for col in null_percentages.select(pl.all())\n",
    "    if col[0] is not None and col[0] > 0.90\n",
    "]\n",
    "if cols_to_drop_step3:\n",
    "    print(f\"Dropping high-null columns: {cols_to_drop_step3}\")\n",
    "    df = df.drop(cols_to_drop_step3)\n",
    "else:\n",
    "    print(\"No columns found with >90% null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Nested Null Percentages\n",
    "\n",
    "**What:** Calculate and display the percentage of null values for specific fields *within* the list structures (`cast`, `directors`, `writers`, `episodes`).\n",
    "**Why:** To understand the data quality inside the nested lists *before* applying imputation. This helps verify if our imputation strategy (using \"missing\"/-1) is reasonable given the amount of missing data.\n",
    "**How:** Define a helper function `analyze_nested_nulls` that uses `explode`, `unnest`, calculates the total number of nested items, and then computes the null percentage for the specified fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4: Inspecting nulls inside list structures ---\") # Renumbered from 4.5\n",
    "\n",
    "def analyze_nested_nulls(df, list_col, fields_to_check):\n",
    "    \"\"\"Helper function to explode, unnest, and calculate null percentages.\"\"\"\n",
    "    if list_col not in df.columns:\n",
    "        print(f\"--- '{list_col}' not found. Skipping analysis. ---\")\n",
    "        return\n",
    "    print(f\"--- '{list_col}' nested null percentages ---\")\n",
    "    try:\n",
    "        # Use LazyFrame for potentially large intermediate result during explode/unnest\n",
    "        df_unnested_lazy = df.lazy().explode(list_col).unnest(list_col)\n",
    "        \n",
    "        # Calculate total count efficiently\n",
    "        total_count = df_unnested_lazy.select(pl.count()).collect().item()\n",
    "        \n",
    "        if total_count == 0:\n",
    "            print(\"No data found after exploding.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Total entries: {total_count}\")\n",
    "        \n",
    "        # Check available columns after unnesting\n",
    "        unnested_cols = df_unnested_lazy.columns\n",
    "        fields_exist = [f for f in fields_to_check if f in unnested_cols]\n",
    "        \n",
    "        if not fields_exist:\n",
    "            print(f\"None of the specified fields {fields_to_check} exist in '{list_col}'. Available: {unnested_cols}\")\n",
    "            return\n",
    "            \n",
    "        # Calculate nulls and percentages for existing fields\n",
    "        null_stats_lazy = df_unnested_lazy.select([\n",
    "            (pl.col(field).null_count() / total_count * 100).alias(f\"{field}_null_pct\")\n",
    "            for field in fields_exist\n",
    "        ])\n",
    "        \n",
    "        # Collect the final stats\n",
    "        null_stats = null_stats_lazy.collect()\n",
    "        print(null_stats)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing '{list_col}': {e}\")\n",
    "\n",
    "# Run the analysis for each list column\n",
    "analyze_nested_nulls(df, 'cast', ['job', 'birthYear', 'primaryProfession', 'primaryName'])\n",
    "analyze_nested_nulls(df, 'directors', ['primaryName', 'birthYear', 'deathYear'])\n",
    "analyze_nested_nulls(df, 'writers', ['primaryName', 'birthYear'])\n",
    "analyze_nested_nulls(df, 'episodes', ['seasonNumber', 'episodeNumber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Standardize Null Lists to Empty Lists\n",
    "\n",
    "**What:** Replace any `null` values in list-type columns with empty lists (`[]`).\n",
    "**Why:** Ensures consistency. An empty list (`[]`) and a `null` list conceptually mean \"no items\", but `null` can cause errors in list processing functions. Using `[]` is safer and maintains the list data type.\n",
    "**How:** Identify list columns and use `fill_null([])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 5: Standardizing null lists to empty lists [] ---\")\n",
    "list_cols = [col for col in df.columns if df[col].dtype == pl.List]\n",
    "expressions = []\n",
    "for col_name in list_cols:\n",
    "    if col_name in df.columns:\n",
    "        expressions.append(pl.col(col_name).fill_null([]))\n",
    "if expressions:\n",
    "    df = df.with_columns(expressions)\n",
    "    print(f\"Standardized nulls for: {list_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sanitize & Standardize Top-Level Columns\n",
    "\n",
    "**What:** Apply various cleaning operations to top-level columns:\n",
    "* **Text:** Trim whitespace, convert to lowercase (or uppercase for regions).\n",
    "* **Genres:** Split, trim, lowercase, sort alphabetically, and rejoin to handle order differences (e.g., `crime,drama` vs `drama,crime`).\n",
    "* **Years:** Convert `endYear` to integer, fill nulls with -1, and sanitize `startYear` and `endYear` by setting impossible dates (e.g., future years, `endYear < startYear`) to -1.\n",
    "* **Runtime:** Fill nulls with -1 and sanitize by setting implausible runtimes (e.g., <= 1 min) to -1.\n",
    "**Why:** To ensure consistency in text formatting, handle data type issues, and correct obvious data entry errors or illogical values.\n",
    "**How:** Using Polars string functions (`.str`), type conversions (`.str.to_integer`), `.map_elements()` for custom logic, and conditional logic (`pl.when`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 6: Sanitizing top-level text, numerics, and data types ---\")\n",
    "current_year = datetime.datetime.now().year + 1 # Add 1 year buffer\n",
    "standardize_exprs = []\n",
    "\n",
    "if 'genres' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('genres').fill_null(\"missing\").str.strip_chars().str.to_lowercase()\n",
    "          .map_elements(lambda s: \",\".join(sorted([part.strip() for part in s.split(',')])), return_dtype=pl.String)\n",
    "          .alias('genres')\n",
    "    )\n",
    "if 'titleType' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('titleType').fill_null(\"missing\").str.strip_chars().str.to_lowercase().alias('titleType')\n",
    "    )\n",
    "if 'language' in df.columns:\n",
    "     standardize_exprs.append(\n",
    "        pl.col('language').fill_null(\"missing\").str.strip_chars().str.to_lowercase().alias('language')\n",
    "    )\n",
    "if 'region' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('region').fill_null(\"missing\").str.strip_chars().str.to_uppercase().alias('region')\n",
    "    )\n",
    "if 'endYear' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('endYear').str.to_integer(strict=False).fill_null(-1).alias('endYear')\n",
    "    )\n",
    "if 'startYear' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('startYear').fill_null(-1)\n",
    "          .map_elements(lambda y: -1 if (y is not None and (y > current_year or (y < 1870 and y != -1))) else y, return_dtype=pl.Int64)\n",
    "          .alias('startYear')\n",
    "    )\n",
    "if 'runtimeMinutes' in df.columns:\n",
    "    standardize_exprs.append(\n",
    "        pl.col('runtimeMinutes').fill_null(-1)\n",
    "          .map_elements(lambda r: -1 if (r is not None and (r <= 1 or r > 30000)) else r, return_dtype=pl.Int64)\n",
    "          .alias('runtimeMinutes')\n",
    "    )\n",
    "\n",
    "if standardize_exprs:\n",
    "    df = df.with_columns(standardize_exprs)\n",
    "    print(\"Top-level text, numeric, and type sanitization complete.\")\n",
    "\n",
    "# Fix illogical date ranges (endYear < startYear)\n",
    "if 'endYear' in df.columns and 'startYear' in df.columns:\n",
    "    df = df.with_columns(\n",
    "        pl.when(\n",
    "            (pl.col('endYear') < pl.col('startYear')) &\n",
    "            (pl.col('endYear') != -1) &\n",
    "            (pl.col('startYear') != -1)\n",
    "        )\n",
    "        .then(-1)\n",
    "        .otherwise(pl.col('endYear'))\n",
    "        .alias('endYear')\n",
    "    )\n",
    "    print(\"Fixed illogical date ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Harmonize Categories, Fix Logic & Show Outliers\n",
    "\n",
    "**What:**\n",
    "* Harmonize spellings/capitalization for `titleType` (e.g., 'tvseries' -> 'tvSeries').\n",
    "* Fix logical inconsistency: Change `titleType` from 'movie' to 'tvSeries' if a movie incorrectly has associated episodes.\n",
    "* Identify and display rows that are potential outliers based on defined thresholds for runtime, vote count, and start year.\n",
    "\n",
    "**Why:**\n",
    "* Ensures categorical consistency.\n",
    "* Corrects data errors based on logical rules.\n",
    "* Allows for manual inspection of potentially erroneous or unusual data points without altering them at this stage.\n",
    "\n",
    "**How:** Using `.replace()` with a mapping dictionary, conditional logic (`pl.when`), list length checks (`.list.len()`), and filtering based on combined conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Harmonizing categories, fixing logic & showing outliers ---\")\n",
    "if 'titleType' in df.columns:\n",
    "    titletype_map = {\n",
    "        \"tvseries\": \"tvSeries\", \"tvepisode\": \"tvEpisode\", \"tvspecial\": \"tvSpecial\",\n",
    "        \"tvshort\": \"tvShort\", \"tvmovie\": \"tvMovie\", \"movie\": \"movie\"\n",
    "    }\n",
    "    df = df.with_columns(\n",
    "        pl.col('titleType').replace(titletype_map, default=\"other\").alias('titleType')\n",
    "    )\n",
    "    print(\"Harmonized 'titleType'.\")\n",
    "\n",
    "# Fix Logical Inconsistencies\n",
    "if 'titleType' in df.columns and 'episodes' in df.columns:\n",
    "    # Check if 'episodes' column is not entirely null or empty lists first\n",
    "    if not df.select(pl.col('episodes').list.len().sum()).item() == 0:\n",
    "        movie_with_episodes = (pl.col('titleType') == 'movie') & (pl.col('episodes').list.len() > 0)\n",
    "        df = df.with_columns(\n",
    "            pl.when(movie_with_episodes)\n",
    "            .then(pl.lit('tvSeries'))\n",
    "            .otherwise(pl.col('titleType'))\n",
    "            .alias('titleType')\n",
    "        )\n",
    "        print(\"Corrected 'movie' entries that had episode lists to 'tvSeries'.\")\n",
    "    else:\n",
    "        print(\"Skipping 'movie' with episodes check as 'episodes' column seems empty.\")\n",
    "\n",
    "# Show Potential Outliers\n",
    "print(\"\\n--- Showing potential outlier rows ---\")\n",
    "MAX_RUNTIME_MINS = 600\n",
    "MAX_VOTES = 5_000_000\n",
    "MAX_YEAR = datetime.datetime.now().year + 5\n",
    "\n",
    "outlier_conditions = []\n",
    "if 'runtimeMinutes' in df.columns:\n",
    "    outlier_conditions.append(\n",
    "        (pl.col('runtimeMinutes') > MAX_RUNTIME_MINS) & (pl.col('runtimeMinutes') != -1)\n",
    "    )\n",
    "if 'numVotes' in df.columns:\n",
    "     outlier_conditions.append(\n",
    "        (pl.col('numVotes') > MAX_VOTES) & (pl.col('numVotes') != -1)\n",
    "     )\n",
    "if 'startYear' in df.columns:\n",
    "     outlier_conditions.append(\n",
    "        (pl.col('startYear') > MAX_YEAR) & (pl.col('startYear') != -1)\n",
    "     )\n",
    "\n",
    "if outlier_conditions:\n",
    "    combined_condition = reduce(operator.or_, outlier_conditions)\n",
    "    outlier_rows = df.filter(combined_condition)\n",
    "\n",
    "    if outlier_rows.height > 0:\n",
    "        print(f\"Found {outlier_rows.height} rows flagged as potential outliers. Showing first 10:\")\n",
    "        cols_to_show = [col for col in ['startYear', 'runtimeMinutes', 'numVotes'] if col in df.columns]\n",
    "        if 'titleType' in df.columns: cols_to_show = ['titleType'] + cols_to_show # Add context\n",
    "        print(outlier_rows.select(cols_to_show).head(10))\n",
    "    else:\n",
    "        print(\"No rows flagged as outliers.\")\n",
    "else:\n",
    "    print(\"No outlier conditions could be checked (columns missing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Comprehensive Nested List Cleaning\n",
    "\n",
    "**What:** Apply detailed cleaning *inside* the list structures (`cast`, `directors`, `writers`, `episodes`):\n",
    "* **Drop Nested IDs:** Implicitly drop `ordering`, `nconst`, `tconst` by rebuilding structs without them.\n",
    "* **Impute Nulls:** Fill nulls with \"missing\" (string) or -1 (numeric).\n",
    "* **Standardize Text:** Trim whitespace, lowercase relevant fields.\n",
    "* **Harmonize Categories:** Map `cast.category` ('actress'->'actor'). Map `cast.job` using the `map_job` function. Map `cast.primaryProfession` using the `clean_professions` function (handles comma-separated values).\n",
    "* **Extract Metadata:** Create boolean flags (`isVoiceRole`, `isUncredited`, `isArchive`) from `cast.characters` and clean the original `characters` string.\n",
    "* **Standardize Names:** Remove Roman numeral suffixes (e.g., `(I)`, `(II)`) from `primaryName` fields.\n",
    "**Why:** This is the core cleaning step for the complex nested data, ensuring consistency, fixing errors, extracting valuable features, and preparing the data for potential flattening or feature engineering later.\n",
    "**How:** Primarily using `list.eval()` combined with `pl.struct()` to rebuild the nested structs. Helper Python functions (`map_job`, `clean_professions`) are applied using `.map_elements()`. String manipulation (`.str`), regex (`.str.contains`, `.str.replace_all`), and conditional logic are used extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 8: Advanced cleaning of all nested lists ---\")\n",
    "# --- Profession Map ---\n",
    "profession_map = {\n",
    "    \"actor\": \"actor\", \"actress\": \"actor\", \"director\": \"director\", \"writer\": \"writer\",\n",
    "    \"producer\": \"producer\", \"composer\": \"composer\", \"cinematographer\": \"cinematographer\",\n",
    "    \"editor\": \"editor\", \"casting_director\": \"casting_director\", \"casting_department\": \"casting_director\",\n",
    "    \"production_designer\": \"production_designer\", \"art_director\": \"art_department\",\n",
    "    \"set_decorator\": \"art_department\", \"art_department\": \"art_department\",\n",
    "    \"costume_designer\": \"costume_designer\", \"costume_department\": \"costume_designer\",\n",
    "    \"make_up_department\": \"make_up_department\", \"sound_department\": \"sound_crew\",\n",
    "    \"music_department\": \"sound_crew\", \"camera_department\": \"camera_crew\",\n",
    "    \"editorial_department\": \"editorial_crew\", \"animation_department\": \"vfx_animation_crew\",\n",
    "    \"visual_effects\": \"vfx_animation_crew\", \"special_effects\": \"vfx_animation_crew\",\n",
    "    \"assistant_director\": \"production_crew\", \"production_manager\": \"production_crew\",\n",
    "    \"production_department\": \"production_crew\", \"location_management\": \"production_crew\",\n",
    "    \"transportation_department\": \"production_crew\", \"script_department\": \"production_crew\",\n",
    "    \"stunts\": \"stunts\", \"soundtrack\": \"soundtrack\", \"archive_footage\": \"archive_footage\",\n",
    "    \"miscellaneous\": \"other\", \"talent_agent\": \"other_business\", \"manager\": \"other_business\",\n",
    "    \"publicist\": \"other_business\", \"legal\": \"other_business\", \"executive\": \"other_business\",\n",
    "}\n",
    "def clean_professions(prof_string, mapping):\n",
    "    if prof_string is None or prof_string == \"missing\": return \"missing\"\n",
    "    professions = prof_string.split(',')\n",
    "    mapped_professions = set(mapping.get(prof.strip(), \"other\") for prof in professions)\n",
    "    return \",\".join(sorted(list(mapped_professions)))\n",
    "\n",
    "# --- Job Map ---\n",
    "def map_job(job_str):\n",
    "    if job_str is None or job_str == \"missing\" or job_str.strip() == \"\": return \"missing\"\n",
    "    job_str = job_str.lower()\n",
    "    if \"screenplay\" in job_str or \"screen play\" in job_str: return \"screenplay\"\n",
    "    if \"story\" in job_str: return \"story\"\n",
    "    if \"writer\" in job_str or \"written by\" in job_str or \"scenario\" in job_str: return \"writer\"\n",
    "    if \"adaptation\" in job_str or \"dialogue\" in job_str or \"script\" in job_str: return \"writer\"\n",
    "    if \"director of photography\" in job_str or \"cinematographer\" in job_str: return \"cinematographer/dp\"\n",
    "    if \"director\" in job_str: return \"director\"\n",
    "    if \"executive producer\" in job_str: return \"executive_producer\"\n",
    "    if \"line producer\" in job_str: return \"line_producer\"\n",
    "    if \"producer\" in job_str: return \"producer\"\n",
    "    if \"composer\" in job_str: return \"composer\"\n",
    "    if \"editor\" in job_str or \"film editor\" in job_str: return \"editor\"\n",
    "    if \"casting_director\" in job_str: return \"casting_director\"\n",
    "    if \"production_designer\" in job_str: return \"production_designer\"\n",
    "    if \"novel\" in job_str or \"book\" in job_str or \"manga\" in job_str: return \"source_material (novel/book)\"\n",
    "    if \"play\" in job_str: return \"source_material (play)\"\n",
    "    if \"characters\" in job_str or \"created by\" in job_str or \"creator\" in job_str: return \"creator\"\n",
    "    if \"based on\" in job_str: return \"source_material (based on)\"\n",
    "    if \"titles\" in job_str: return \"titles\"\n",
    "    if \"idea\" in job_str: return \"idea\"\n",
    "    return \"other\"\n",
    "\n",
    "cleaning_expressions = []\n",
    "\n",
    "if 'cast' in df.columns:\n",
    "    print(\"\\nCleaning 'cast' (with metadata extraction)...\")\n",
    "    cleaning_expressions.append(\n",
    "        pl.col('cast').list.eval(\n",
    "            pl.struct([\n",
    "                pl.element().struct.field('category').fill_null(\"missing\").str.strip_chars().str.to_lowercase().replace({\"actress\": \"actor\"}).alias('category'),\n",
    "                pl.element().struct.field('job').fill_null(\"missing\").str.strip_chars()\n",
    "                    .map_elements(map_job, return_dtype=pl.String)\n",
    "                    .alias('job'),\n",
    "                pl.element().struct.field('characters').str.contains(r\"\\(voice\\)\").fill_null(False).alias(\"isVoiceRole\"),\n",
    "                pl.element().struct.field('characters').str.contains(r\"\\(uncredited\\)\").fill_null(False).alias(\"isUncredited\"),\n",
    "                pl.element().struct.field('characters').str.contains(r\"\\(archive footage\\)\").fill_null(False).alias(\"isArchive\"),\n",
    "                pl.element().struct.field('characters').fill_null(\"missing\").str.strip_chars()\n",
    "                    .str.replace_all(r\"\\(voice\\)\", \"\").str.replace_all(r\"\\(uncredited\\)\", \"\").str.replace_all(r\"\\(archive footage\\)\", \"\")\n",
    "                    .str.replace_all(r\"\\s*\\([IVXLCDM]+\\)$\", \"\")\n",
    "                    .str.strip_chars()\n",
    "                    .alias('characters'),\n",
    "                pl.element().struct.field('primaryName').fill_null(\"missing\").str.strip_chars()\n",
    "                    .str.replace_all(r\"\\s*\\([IVXLCDM]+\\)$\", \"\")\n",
    "                    .alias('primaryName'),\n",
    "                pl.element().struct.field('primaryProfession').fill_null(\"missing\").str.strip_chars().str.to_lowercase()\n",
    "                    .map_elements(lambda s: clean_professions(s, profession_map), return_dtype=pl.String)\n",
    "                    .alias('primaryProfession'),\n",
    "                pl.element().struct.field('birthYear').fill_null(-1).alias('birthYear'),\n",
    "                pl.element().struct.field('deathYear').fill_null(-1).alias('deathYear'),\n",
    "             ])\n",
    "        ).alias('cast')\n",
    "    )\n",
    "\n",
    "if 'directors' in df.columns:\n",
    "    print(\"Cleaning 'directors' (with name standardization)...\")\n",
    "    cleaning_expressions.append(\n",
    "        pl.col('directors').list.eval(\n",
    "             pl.struct([\n",
    "                 pl.element().struct.field('primaryName').fill_null(\"missing\").str.strip_chars()\n",
    "                     .str.replace_all(r\"\\s*\\([IVXLCDM]+\\)$\", \"\")\n",
    "                     .alias('primaryName'),\n",
    "                 pl.element().struct.field('birthYear').fill_null(-1).alias('birthYear'),\n",
    "                 pl.element().struct.field('deathYear').fill_null(-1).alias('deathYear'),\n",
    "             ])\n",
    "        ).alias('directors')\n",
    "    )\n",
    "\n",
    "if 'writers' in df.columns:\n",
    "    print(\"Cleaning 'writers' (with name standardization)...\")\n",
    "    cleaning_expressions.append(\n",
    "        pl.col('writers').list.eval(\n",
    "            pl.struct([\n",
    "                 pl.element().struct.field('primaryName').fill_null(\"missing\").str.strip_chars()\n",
    "                     .str.replace_all(r\"\\s*\\([IVXLCDM]+\\)$\", \"\")\n",
    "                     .alias('primaryName'),\n",
    "                 pl.element().struct.field('birthYear').fill_null(-1).alias('birthYear'),\n",
    "                 pl.element().struct.field('deathYear').fill_null(-1).alias('deathYear'),\n",
    "             ])\n",
    "        ).alias('writers')\n",
    "    )\n",
    "\n",
    "if 'episodes' in df.columns:\n",
    "    print(\"Cleaning 'episodes'...\")\n",
    "    cleaning_expressions.append(\n",
    "        pl.col('episodes').list.eval(\n",
    "           pl.struct([\n",
    "               pl.element().struct.field('seasonNumber').fill_null(-1).alias('seasonNumber'),\n",
    "               pl.element().struct.field('episodeNumber').fill_null(-1).alias('episodeNumber')\n",
    "           ])\n",
    "        ).alias('episodes')\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping 'episodes' cleaning (column may have been dropped).\")\n",
    "\n",
    "if cleaning_expressions:\n",
    "    df = df.with_columns(cleaning_expressions)\n",
    "    print(\"\\nNested list cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Build and Apply Sklearn Pipeline for Imputation\n",
    "\n",
    "**What:** Create and apply an `sklearn` pipeline to perform final imputation on any remaining missing values in top-level **numeric** and **categorical** columns.\n",
    "**Why:** This uses standard placeholder values (-1 for numeric, \"missing\" for categorical) as a final catch-all. Crucially, putting this in an `sklearn` pipeline makes the imputation step reproducible. The same imputation logic (learned from the training set if `.fit()` were used) can be applied consistently to new data (using `.transform()`).\n",
    "**How:** \n",
    "* Identify numeric and categorical columns dynamically.\n",
    "* Use `SimpleImputer(strategy='constant')` within a `ColumnTransformer` to apply the correct placeholder to the correct column types.\n",
    "* Set `remainder='passthrough'` to keep columns not explicitly handled (like list columns or boolean flags).\n",
    "* Apply the pipeline using `.fit_transform()` (in a real scenario, you'd `.fit()` on train and `.transform()` on train/test/new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 9: Building imputation pipeline for top-level columns ---\")\n",
    "numeric_features = [\n",
    "    col for col, dtype in df.schema.items()\n",
    "    if dtype in [pl.Int64, pl.Float64, pl.Int32, pl.Float32] and col not in ['isAdult']\n",
    "]\n",
    "categorical_features = [\n",
    "    col for col, dtype in df.schema.items()\n",
    "    if dtype in [pl.String, pl.Categorical]\n",
    "]\n",
    "print(f\"Numeric features for pipeline: {numeric_features}\")\n",
    "print(f\"Categorical features for pipeline: {categorical_features}\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep list columns, 'isAdult', etc.\n",
    ")\n",
    "\n",
    "cleaning_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "print(\"\\nFinal sklearn pipeline created:\")\n",
    "print(cleaning_pipeline)\n",
    "\n",
    "# --- Apply the pipeline ---\n",
    "try:\n",
    "    df_cleaned = cleaning_pipeline.fit_transform(df)\n",
    "    print(\"\\n--- DataFrame after applying imputation pipeline ---\")\n",
    "    print(df_cleaned.head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not apply pipeline: {e}\")\n",
    "    df_cleaned = df # Keep the pre-pipeline state for inspection\n",
    "\n",
    "print(\"\\n--- Final Cleaned Schema ---\")\n",
    "print(df_cleaned.schema)\n",
    "\n",
    "print(\"\\n--- Final Data Preview ---\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Cleaning Complete\n",
    "\n",
    "The `df_cleaned` DataFrame now contains the processed data. It has undergone:\n",
    "* Removal of identifiers and redundant columns.\n",
    "* Removal of constant-value and high-null columns.\n",
    "* Standardization of text casing and whitespace.\n",
    "* Sanitization of numeric values (years, runtime) and data types.\n",
    "* Harmonization of categorical values (genres, titleType, professions, jobs).\n",
    "* Extraction of metadata from nested fields (character flags).\n",
    "* Standardization of names (removing Roman numerals).\n",
    "* Correction of logical inconsistencies.\n",
    "* Inspection and removal of duplicate rows.\n",
    "* Inspection of potential outliers.\n",
    "* Consistent handling of nulls (using placeholders \"missing\"/-1 and empty lists []).\n",
    "\n",
    "The data should now be significantly cleaner and more consistent, ready for further feature engineering or modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}