{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f17d71ce",
   "metadata": {},
   "source": [
    "# DATA LOADING + DATA CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_chunked_merge_azure.py\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# =======================\n",
    "# 1️⃣ Azure URLs for your IMDb .tsv.gz files\n",
    "# Replace <account>, <container>, and SAS token if needed\n",
    "# =======================\n",
    "URL_NAME_BASICS    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104122_UTC/name.basics.tsv.gz\"\n",
    "URL_TITLE_AKAS     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104546_UTC/title.akas.tsv.gz\"\n",
    "URL_TITLE_BASICS   = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104810_UTC/title.basics.tsv.gz\"\n",
    "URL_TITLE_CREW     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104937_UTC/title.crew.tsv.gz\"\n",
    "URL_TITLE_EPISODE  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105103_UTC/title.episode.tsv.gz\"\n",
    "URL_TITLE_PRINC    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105225_UTC/title.principals.tsv.gz\"\n",
    "URL_TITLE_RATINGS  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/title.ratings.tsv.gz\"\n",
    "\n",
    "# =======================\n",
    "# 2️⃣ Helper function to read TSV from URL\n",
    "# =======================\n",
    "def load_tsv(url, **kwargs):\n",
    "    return pd.read_csv(url, sep='\\t', na_values='\\\\N', compression='gzip', low_memory=False, **kwargs)\n",
    "\n",
    "# =======================\n",
    "# 3️⃣ Load smaller tables into memory first\n",
    "# These are manageable in RAM: basics, ratings, crew, episode, name.basics\n",
    "# =======================\n",
    "print(\"Loading smaller datasets...\")\n",
    "title_basics = load_tsv(URL_TITLE_BASICS)\n",
    "title_ratings = load_tsv(URL_TITLE_RATINGS)\n",
    "title_crew = load_tsv(URL_TITLE_CREW)\n",
    "title_episode = load_tsv(URL_TITLE_EPISODE)\n",
    "name_basics = load_tsv(URL_NAME_BASICS)\n",
    "\n",
    "# =======================\n",
    "# 4️⃣ Merge basics + ratings + crew + episode\n",
    "# This creates a master table: one row per title (tconst)\n",
    "# =======================\n",
    "print(\"Merging basics + ratings + crew + episode...\")\n",
    "merged = (\n",
    "    title_basics\n",
    "    .merge(title_ratings, on='tconst', how='left')\n",
    "    .merge(title_crew, on='tconst', how='left')\n",
    "    .merge(title_episode, on='tconst', how='left')\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del title_basics, title_ratings, title_crew, title_episode\n",
    "gc.collect()\n",
    "\n",
    "# =======================\n",
    "# 5️⃣ Build set of all tconst\n",
    "# This helps filter only relevant rows in big files (akas and principals)\n",
    "# =======================\n",
    "tconst_set = set(merged['tconst'].unique())\n",
    "print(f\"Master table has {len(tconst_set)} unique titles.\")\n",
    "\n",
    "# =======================\n",
    "# 6️⃣ Process title.akas in chunks\n",
    "# Only keep rows where titleId is in tconst_set\n",
    "# =======================\n",
    "print(\"Processing title.akas in chunks...\")\n",
    "akas_cols = ['titleId','ordering','title','region','language','types','attributes','isOriginalTitle']\n",
    "akas_chunks = pd.read_csv(URL_TITLE_AKAS, sep='\\t', na_values='\\\\N', compression='gzip',\n",
    "                          usecols=akas_cols, chunksize=300_000, low_memory=False)\n",
    "akas_parts = []\n",
    "for chunk in akas_chunks:\n",
    "    chunk = chunk[chunk['titleId'].isin(tconst_set)]  # filter relevant titles\n",
    "    if not chunk.empty:\n",
    "        akas_parts.append(chunk)\n",
    "    del chunk\n",
    "gc.collect()\n",
    "\n",
    "if akas_parts:\n",
    "    akas_df = pd.concat(akas_parts, ignore_index=True)\n",
    "    # Merge: many-to-one relationship (one tconst may have multiple akas)\n",
    "    merged = merged.merge(akas_df, left_on='tconst', right_on='titleId', how='left')\n",
    "    del akas_df, akas_parts\n",
    "gc.collect()\n",
    "\n",
    "# =======================\n",
    "# 7️⃣ Process title.principals in chunks\n",
    "# Only keep rows where tconst exists in master table\n",
    "# =======================\n",
    "print(\"Processing title.principals in chunks...\")\n",
    "princ_cols = ['tconst','ordering','nconst','category','job','characters']\n",
    "princ_chunks = pd.read_csv(URL_TITLE_PRINC, sep='\\t', na_values='\\\\N', compression='gzip',\n",
    "                           usecols=princ_cols, chunksize=300_000, low_memory=False)\n",
    "princ_parts = []\n",
    "for chunk in princ_chunks:\n",
    "    chunk = chunk[chunk['tconst'].isin(tconst_set)]  # filter relevant titles\n",
    "    if not chunk.empty:\n",
    "        princ_parts.append(chunk)\n",
    "    del chunk\n",
    "gc.collect()\n",
    "\n",
    "if princ_parts:\n",
    "    principals_df = pd.concat(princ_parts, ignore_index=True)\n",
    "    # Merge: many-to-one (explodes rows if multiple principals per title)\n",
    "    merged = merged.merge(principals_df, on='tconst', how='left')\n",
    "    del principals_df, princ_parts\n",
    "gc.collect()\n",
    "\n",
    "# =======================\n",
    "# 8️⃣ Merge name.basics on nconst to get person info for principals\n",
    "# =======================\n",
    "print(\"Merging name.basics for principals...\")\n",
    "merged = merged.merge(name_basics, on='nconst', how='left')\n",
    "del name_basics\n",
    "gc.collect()\n",
    "\n",
    "# =======================\n",
    "# 9️⃣ Final dataset info\n",
    "# =======================\n",
    "print(\"Final merged shape:\", merged.shape)\n",
    "print(merged.info(memory_usage='deep'))\n",
    "\n",
    "# =======================\n",
    "# 10️⃣ Save final merged dataset\n",
    "# Save as Parquet (recommended) and compressed CSV (optional)\n",
    "# =======================\n",
    "print(\"Saving merged dataset...\")\n",
    "merged.to_parquet(\"imdb_merged_chunked.parquet\", engine='pyarrow', compression='snappy', index=False)\n",
    "print(\"✅ Done! You can reload Parquet fast with: pd.read_parquet('imdb_merged_chunked.parquet')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "#  Azure URLs\n",
    "# =======================\n",
    "URL_NAME_BASICS    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104122_UTC/name.basics.tsv.gz\"\n",
    "URL_TITLE_AKAS     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104546_UTC/title.akas.tsv.gz\"\n",
    "URL_TITLE_BASICS   = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104810_UTC/title.basics.tsv.gz\"\n",
    "URL_TITLE_CREW     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104937_UTC/title.crew.tsv.gz\"\n",
    "URL_TITLE_EPISODE  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105103_UTC/title.episode.tsv.gz\"\n",
    "URL_TITLE_PRINC    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105225_UTC/title.principals.tsv.gz\"\n",
    "URL_TITLE_RATINGS  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/title.ratings.tsv.gz\"\n",
    "\n",
    "# =======================\n",
    "#  Start DuckDB connection\n",
    "# =======================\n",
    "con = duckdb.connect(database=':memory:')\n",
    "start_time = time.time()\n",
    "print(\" Starting IMDb data merge using DuckDB...\\n\")\n",
    "\n",
    "# =======================\n",
    "#  Register TSV.GZ files as virtual tables\n",
    "# =======================\n",
    "print(\"1. Registering remote files as views...\")\n",
    "base_read_csv = \"SELECT * FROM read_csv('{}', delim='\\\\t', nullstr='\\\\\\\\N', header=True, compression='gzip', auto_detect=True, parallel=True)\"\n",
    "\n",
    "con.execute(f\"CREATE OR REPLACE VIEW name_basics AS {base_read_csv.format(URL_NAME_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_basics AS {base_read_csv.format(URL_TITLE_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_ratings AS {base_read_csv.format(URL_TITLE_RATINGS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_crew AS {base_read_csv.format(URL_TITLE_CREW)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_episode AS {base_read_csv.format(URL_TITLE_EPISODE)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_akas AS {base_read_csv.format(URL_TITLE_AKAS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_principals AS {base_read_csv.format(URL_TITLE_PRINC)};\")\n",
    "\n",
    "# =======================\n",
    "#  ⭐️ NEW: Create a FILTERED view of title_basics\n",
    "# =======================\n",
    "print(\"2. Filtering title_basics to main titles (movie, tvSeries, etc.)...\")\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW basics_filtered AS\n",
    "SELECT *\n",
    "FROM title_basics\n",
    "WHERE titleType IN ('movie', 'tvSeries', 'tvMiniSeries', 'tvSpecial');\n",
    "\"\"\")\n",
    "count_result = con.execute(\"SELECT COUNT(*) FROM basics_filtered\").fetchone()\n",
    "print(f\"   Filtered down to {count_result[0]:,} titles (from ~12 million).\")\n",
    "\n",
    "# =======================\n",
    "#  NEW: Run ONE optimized query\n",
    "# =======================\n",
    "print(\"3. Running optimized join query...\")\n",
    "\n",
    "# This single, explicit query avoids all duplicate columns\n",
    "final_query = \"\"\"\n",
    "COPY (\n",
    "    SELECT \n",
    "        -- From title_basics (aliased as 'b')\n",
    "        b.tconst, \n",
    "        b.titleType, \n",
    "        b.primaryTitle, \n",
    "        b.originalTitle, \n",
    "        b.isAdult, \n",
    "        b.startYear, \n",
    "        b.endYear, \n",
    "        b.runtimeMinutes, \n",
    "        b.genres,\n",
    "        \n",
    "        -- From title_ratings (aliased as 'r')\n",
    "        r.averageRating, \n",
    "        r.numVotes,\n",
    "        \n",
    "        -- From title_crew (aliased as 'c')\n",
    "        c.directors, \n",
    "        c.writers,\n",
    "        \n",
    "        -- From title_episode (aliased as 'e')\n",
    "        e.parentTconst, \n",
    "        e.seasonNumber, \n",
    "        e.episodeNumber,\n",
    "        \n",
    "        -- From title_akas (aliased as 'a')\n",
    "        a.ordering AS ordering_akas, \n",
    "        a.title AS title_akas, \n",
    "        a.region, \n",
    "        a.language, \n",
    "        a.types, \n",
    "        a.attributes, \n",
    "        a.isOriginalTitle,\n",
    "        \n",
    "        -- From title_principals (aliased as 'p')\n",
    "        p.ordering AS ordering_principal, \n",
    "        p.nconst, \n",
    "        p.category, \n",
    "        p.job, \n",
    "        p.characters,\n",
    "        \n",
    "        -- From name_basics (aliased as 'n')\n",
    "        n.primaryName, \n",
    "        n.birthYear, \n",
    "        n.deathYear, \n",
    "        n.primaryProfession, \n",
    "        n.knownForTitles\n",
    "\n",
    "    FROM basics_filtered AS b\n",
    "    LEFT JOIN title_ratings AS r ON b.tconst = r.tconst\n",
    "    LEFT JOIN title_crew AS c ON b.tconst = c.tconst\n",
    "    LEFT JOIN title_episode AS e ON b.tconst = e.tconst\n",
    "    LEFT JOIN title_akas AS a ON b.tconst = a.titleId\n",
    "    LEFT JOIN title_principals AS p ON b.tconst = p.tconst\n",
    "    LEFT JOIN name_basics AS n ON p.nconst = n.nconst\n",
    ") \n",
    "TO 'imdb_merged_duckdb_FILTERED.parquet' (FORMAT PARQUET, COMPRESSION 'SNAPPY', ROW_GROUP_SIZE 100000);\n",
    "\"\"\"\n",
    "\n",
    "con.execute(final_query)\n",
    "\n",
    "# =======================\n",
    "#  Clean up\n",
    "# =======================\n",
    "con.close()\n",
    "elapsed = (time.time() - start_time)\n",
    "print(f\"\\n✅ Done! Merged dataset saved as imdb_merged_duckdb_FILTERED.parquet (Elapsed: {elapsed:.2f} seconds)\")\n",
    "print(\"   This file is filtered and ready for your cleaning pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "#  Azure URLs\n",
    "# =======================\n",
    "URL_NAME_BASICS    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104122_UTC/name.basics.tsv.gz\"\n",
    "URL_TITLE_AKAS     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104546_UTC/title.akas.tsv.gz\"\n",
    "URL_TITLE_BASICS   = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104810_UTC/title.basics.tsv.gz\"\n",
    "URL_TITLE_CREW     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104937_UTC/title.crew.tsv.gz\"\n",
    "URL_TITLE_EPISODE  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105103_UTC/title.episode.tsv.gz\"\n",
    "URL_TITLE_PRINC    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105225_UTC/title.principals.tsv.gz\"\n",
    "URL_TITLE_RATINGS  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/title.ratings.tsv.gz\"\n",
    "\n",
    "# =======================\n",
    "#  Start DuckDB connection\n",
    "# =======================\n",
    "# Connect to an in-memory database or specify a file: database='imdb_merge.duckdb'\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "start_time = time.time()\n",
    "print(\" Starting IMDb data merge using DuckDB...\\n\")\n",
    "\n",
    "# =======================\n",
    "#  Stage progress tracker\n",
    "# =======================\n",
    "stages = [\n",
    "    \"Registering IMDb files\",\n",
    "    \"Joining basics + ratings + crew + episode\",\n",
    "    \"Joining akas\",\n",
    "    \"Joining principals\",\n",
    "    \"Joining name.basics\",\n",
    "    \"Exporting to Parquet\"\n",
    "]\n",
    "progress = tqdm(total=len(stages), desc=\"Progress\", ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
    "\n",
    "# =======================\n",
    "#  Register TSV.GZ files as virtual tables\n",
    "# =======================\n",
    "# DuckDB can read directly from HTTPS URLs and handle compressed files.\n",
    "# 'auto_detect=True' helps with schema, but we specify key params.\n",
    "base_read_csv = \"SELECT * FROM read_csv('{}', delim='\\\\t', nullstr='\\\\\\\\N', header=True, compression='gzip', auto_detect=True, parallel=True)\"\n",
    "\n",
    "con.execute(f\"CREATE OR REPLACE VIEW name_basics AS {base_read_csv.format(URL_NAME_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_basics AS {base_read_csv.format(URL_TITLE_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_ratings AS {base_read_csv.format(URL_TITLE_RATINGS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_crew AS {base_read_csv.format(URL_TITLE_CREW)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_episode AS {base_read_csv.format(URL_TITLE_EPISODE)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_akas AS {base_read_csv.format(URL_TITLE_AKAS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_principals AS {base_read_csv.format(URL_TITLE_PRINC)};\")\n",
    "\n",
    "progress.update(1)\n",
    "progress.set_description(stages[1])\n",
    "\n",
    "# =======================\n",
    "#  Perform joins step by step\n",
    "# =======================\n",
    "# This step-by-step materialization helps manage memory\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_core AS\n",
    "SELECT *\n",
    "FROM title_basics b\n",
    "LEFT JOIN title_ratings r USING (tconst)\n",
    "LEFT JOIN title_crew c USING (tconst)\n",
    "LEFT JOIN title_episode e USING (tconst);\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[2])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_with_akas AS\n",
    "SELECT *\n",
    "FROM merged_core mc\n",
    "LEFT JOIN title_akas a ON mc.tconst = a.titleId;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[3])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_with_principals AS\n",
    "SELECT *\n",
    "FROM merged_with_akas ma\n",
    "LEFT JOIN title_principals p ON ma.tconst = p.tconst;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[4])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE imdb_final AS\n",
    "SELECT *\n",
    "FROM merged_with_principals mp\n",
    "LEFT JOIN name_basics n ON mp.nconst = n.nconst;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[5])\n",
    "\n",
    "# =======================\n",
    "#  Export to Parquet\n",
    "# =======================\n",
    "con.execute(\"\"\"\n",
    "COPY (SELECT * FROM imdb_final) \n",
    "TO 'imdb_merged_duckdb.parquet' (FORMAT PARQUET, COMPRESSION 'SNAPPY', ROW_GROUP_SIZE 100000);\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "\n",
    "# =======================\n",
    "#  Clean up\n",
    "# =======================\n",
    "con.close()\n",
    "elapsed = (time.time() - start_time)\n",
    "print(f\"\\n✅ Done! Merged dataset saved as imdb_merged_duckdb.parquet (Elapsed: {elapsed:.2f} seconds)\")\n",
    "print(\"   Reload it fast with: pd.read_parquet('imdb_merged_duckdb.parquet')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231dfdc",
   "metadata": {},
   "source": [
    "# Brief Summary of Our Whole Process of Merging IMDb Datasets\n",
    "\n",
    "## 1. GOAL\n",
    "Merge all datasets into one master table, so that:\n",
    "- Each row contains all information for a title (or exploded rows for multiple actors or alternate titles).\n",
    "- Later, We can clean and analyze the data.\n",
    "\n",
    "## 2. CHALLENGES\n",
    "- Some files (`title.akas` and `title.principals`) are very large (5–10 GB).\n",
    "- Merging everything at once may use too much RAM and crash the computer.\n",
    "- Many rows in `akas` and `principals` may not be relevant to the titles in the main dataset.\n",
    "\n",
    "## 3. APPROACH (CHUNKED MERGE)\n",
    "1. Load small datasets into memory first: `title.basics`, `title.ratings`, `title.crew`, `title.episode`, `name.basics`.\n",
    "2. Merge small datasets on `tconst` → master table with one row per title.\n",
    "3. Build a set of relevant titles (`tconst_set`) for fast filtering.\n",
    "4. Process big datasets (`title.akas` and `title.principals`) in chunks (~300k rows), filter by `tconst_set`, then merge.\n",
    "5. Merge `name.basics` to attach actor/director info using `nconst`.\n",
    "6. Save final dataset (Parquet recommended, CSV optional).\n",
    "\n",
    "## 4. HOW MERGING WORKS\n",
    "- `df_left.merge(df_right, left_on=\"colA\", right_on=\"colB\", how=\"left\")`:\n",
    "  - Left table = main/master table.\n",
    "  - Right table = extra info (ratings, actors, alternate titles).\n",
    "  - `how=\"left\"` keeps all rows from the left table; missing matches → NaN.\n",
    "\n",
    "## 5. ADVANTAGES OF CHUNKED MERGE\n",
    "- Memory-efficient – never load huge tables fully.\n",
    "- Fast filtering – only process rows relevant to `tconst_set`.\n",
    "- Safe – reduces risk of crashing.\n",
    "- Complete – preserves all relevant info, including multiple actors and alternate titles.\n",
    "\n",
    "## 6. NOTES AFTER MERGE\n",
    "- Multiple rows per title are normal:\n",
    "  - One row per actor (`title.principals`).\n",
    "  - One row per alternate title (`title.akas`).\n",
    "- Later during data cleaning, you can:\n",
    "  - Deduplicate rows.\n",
    "  - Aggregate actors or alternate titles into lists.\n",
    "  - Normalize genres or other fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5197ce",
   "metadata": {},
   "source": [
    "# First steps on Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f745da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset you created in the first script\n",
    "try:\n",
    "    df_raw = pd.read_parquet('imdb_merged_chunked.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'imdb_merged_chunked.parquet' not found.\")\n",
    "    print(\"Please run your 'imdb_chunked_merge_azure.py' script first.\")\n",
    "    # As a fallback, create a dummy dataframe for the script to run\n",
    "    df_raw = pd.DataFrame({\n",
    "        'tconst': ['tt000001'], 'primaryTitle': ['Movie Title'], 'startYear': [2000],\n",
    "        'averageRating': [8.0], 'numVotes': [100], 'genres': ['Action,Drama'],\n",
    "        'directors': ['nm000001'], 'writers': ['nm000002'], 'parentTconst': [np.nan],\n",
    "        'seasonNumber': [np.nan], 'episodeNumber': [np.nan], 'titleId': ['tt000001'],\n",
    "        'ordering_x': [1], 'title': ['movie title'], 'region': ['US'], 'language': ['en'],\n",
    "        'isOriginalTitle': [0], 'ordering_y': [1], 'nconst': ['nm000003'],\n",
    "        'category': ['actor'], 'job': [np.nan], 'characters': ['[\"Lead Role\"]'],\n",
    "        'primaryName': ['Actor Name'], 'birthYear': [1980], 'deathYear': [np.nan]\n",
    "    })\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"1. DATA INFO\")\n",
    "print(\"=\"*30)\n",
    "# Use verbose=True to see all columns\n",
    "df_raw.info(verbose=True, memory_usage='deep')\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"2. MISSING VALUES (Top 20)\")\n",
    "print(\"=\"*30)\n",
    "missing_values = df_raw.isnull().sum()\n",
    "missing_percent = (missing_values / len(df_raw) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'count': missing_values, 'percent': missing_percent})\n",
    "print(missing_df[missing_df['count'] > 0].sort_values(by='count', ascending=False).head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"3. DUPLICATE ROWS\")\n",
    "print(\"=\"*30)\n",
    "num_dupes = df_raw.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_dupes}\")\n",
    "# Note: Duplicates might be expected if a title has multiple actors/akas\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"4. CATEGORICAL CARDINALITY (Top 20)\")\n",
    "print(\"=\"*30)\n",
    "categorical_cols = df_raw.select_dtypes(include=['object', 'category']).columns\n",
    "cardinality = {col: df_raw[col].nunique() for col in categorical_cols}\n",
    "print(\"Number of unique values in categorical columns:\")\n",
    "# Sort by cardinality (highest first) to find problematic columns\n",
    "sorted_cardinality = sorted(cardinality.items(), key=lambda x: x[1], reverse=True)\n",
    "for col, count in sorted_cardinality[:20]:\n",
    "    print(f\"{col}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"5. NUMERIC DISTRIBUTION\")\n",
    "print(\"=\"*30)\n",
    "# This helps spot outliers and skew\n",
    "numeric_cols = df_raw.select_dtypes(include=np.number).columns\n",
    "print(df_raw[numeric_cols].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80427bbd",
   "metadata": {},
   "source": [
    "# Cleaning Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695abc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. DEFINE CUSTOM TRANSFORMERS\n",
    "# ==================================\n",
    "\n",
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops specified columns.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns, axis=1, errors='ignore')\n",
    "\n",
    "class StringCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Strips whitespace, lowercases, and converts to string.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[col] = X_transformed[col].astype(str).str.strip().str.lower()\n",
    "        return X_transformed\n",
    "\n",
    "class ListStringCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Cleans comma-separated list-strings (e.g., genres).\"\"\"\n",
    "    def __init__(self, columns, separator=','):\n",
    "        self.columns = columns\n",
    "        self.separator = separator\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                # Fill NaNs with empty string before processing\n",
    "                X_transformed[col] = X_transformed[col].fillna('')\n",
    "                # Split, strip, lowercase, and re-join\n",
    "                X_transformed[col] = X_transformed[col].apply(\n",
    "                    lambda s: self.separator.join(\n",
    "                        [item.strip().lower() for item in str(s).split(self.separator)]\n",
    "                    ) if s else '' # Handle empty strings\n",
    "                )\n",
    "        return X_transformed\n",
    "\n",
    "class JSONStringParser(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Parses columns containing string representations of JSON lists.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def _parse(self, item):\n",
    "        if pd.isna(item):\n",
    "            return []\n",
    "        try:\n",
    "            # Safely evaluate the string as a list\n",
    "            parsed_list = json.loads(item)\n",
    "            if isinstance(parsed_list, list):\n",
    "                # Clean and join the list elements\n",
    "                return ','.join([str(i).strip().lower() for i in parsed_list])\n",
    "            return '' # Not a list\n",
    "        except (json.JSONDecodeError, TypeError, SyntaxError):\n",
    "            return '' # Return empty string on parsing error\n",
    "            \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[col] = X_transformed[col].apply(self._parse)\n",
    "        return X_transformed\n",
    "\n",
    "class CustomOutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Removes rows based on Z-score of specified numeric columns.\"\"\"\n",
    "    def __init__(self, columns, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.columns = columns\n",
    "        self._outliers = None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Ensure columns exist and are numeric\n",
    "        valid_cols = [col for col in self.columns if col in X.columns and pd.api.types.is_numeric_dtype(X[col])]\n",
    "        if not valid_cols:\n",
    "            print(\"Warning: No valid numeric columns found for outlier removal.\")\n",
    "            return X_transformed\n",
    "            \n",
    "        z_scores = np.abs(stats.zscore(X_transformed[valid_cols], nan_policy='omit'))\n",
    "        # Create a boolean mask for rows *without* outliers\n",
    "        mask = (z_scores < self.threshold).all(axis=1)\n",
    "        self._outliers = X_transformed[~mask]\n",
    "        return X_transformed[mask]\n",
    "    @property\n",
    "    def outliers(self):\n",
    "        return self._outliers\n",
    "\n",
    "# ==================================\n",
    "# 2. DEFINE COLUMN GROUPS\n",
    "# (Based on the merge script)\n",
    "# ==================================\n",
    "\n",
    "# Columns to remove: Redundant IDs, low-value, or messy\n",
    "DROP_COLS = [\n",
    "    'titleId',       # Redundant with tconst\n",
    "    'ordering_x',    # ordering from akas\n",
    "    'ordering_y',    # ordering from principals\n",
    "    'isOriginalTitle', # This often appears twice (e.g., isOriginalTitle_x, isOriginalTitle_y)\n",
    "    'attributes',    # Often sparse or noisy\n",
    "    'job'            # Often sparse, 'category' is more useful\n",
    "]\n",
    "\n",
    "# Simple strings to clean (lowercase, strip)\n",
    "CLEAN_STRING_COLS = [\n",
    "    'primaryTitle',\n",
    "    'originalTitle',\n",
    "    'title',\n",
    "    'primaryName'\n",
    "]\n",
    "\n",
    "# Numeric columns to impute (with median)\n",
    "NUMERIC_IMPUTE_COLS = [\n",
    "    'startYear',\n",
    "    'endYear',\n",
    "axr'runtimeMinutes',\n",
    "    'averageRating',\n",
    "    'numVotes',\n",
    "    'seasonNumber',\n",
    "    'episodeNumber',\n",
    "    'birthYear',\n",
    "    'deathYear'\n",
    "]\n",
    "\n",
    "# Categorical columns to impute (with 'unknown')\n",
    "CATEGORICAL_IMPUTE_COLS = [\n",
    "    'titleType',\n",
    "    'isAdult',\n",
    "    'region',\n",
    "    'language',\n",
    "    'types',\n",
    "    'category'\n",
    "]\n",
    "\n",
    "# Comma-separated list-strings\n",
    "LIST_STRING_COLS = [\n",
    "    'genres',\n",
    "    'directors',\n",
    "    'writers',\n",
    "    'primaryProfession',\n",
    "    'knownForTitles'\n",
    "]\n",
    "\n",
    "# JSON-like list-strings\n",
    "JSON_STRING_COLS = [\n",
    "    'characters'\n",
    "]\n",
    "\n",
    "# Numeric columns to check for outliers\n",
    "OUTLIER_COLS = [\n",
    "    'runtimeMinutes',\n",
    "    'numVotes',\n",
    "    'averageRating',\n",
    "    'startYear'\n",
    "]\n",
    "\n",
    "# ==================================\n",
    "# 3. BUILD THE PREPROCESSING PIPELINE\n",
    "# ==================================\n",
    "\n",
    "# Define imputers\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "\n",
    "# Create the main preprocessing pipeline using ColumnTransformer\n",
    "# This applies specific transformers to specific column groups\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('drop_cols', DropColumnTransformer(columns=DROP_COLS)),\n",
    "    \n",
    "    ('clean_strings', StringCleaner(columns=CLEAN_STRING_COLS)),\n",
    "    \n",
    "    ('clean_list_strings', ListStringCleaner(columns=LIST_STRING_COLS)),\n",
    "    \n",
    "    ('parse_json_strings', JSONStringParser(columns=JSON_STRING_COLS)),\n",
    "    \n",
    "    ('impute_features', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_impute', numeric_imputer, NUMERIC_IMPUTE_COLS),\n",
    "            ('cat_impute', categorical_imputer, CATEGORICAL_IMPUTE_COLS)\n",
    "        ],\n",
    "        remainder='passthrough' # Keep all other columns not specified\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ⚠️ WARNING: Outlier removal should generally be done *ONLY* on the \n",
    "# training set *after* splitting to avoid data leakage.\n",
    "# We define it separately.\n",
    "outlier_remover = CustomOutlierRemover(columns=OUTLIER_COLS, threshold=3)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# 4. EXAMPLE USAGE\n",
    "# ==================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"6. APPLYING CLEANING PIPELINE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# We use the 'df_raw' loaded in the exploration step\n",
    "print(f\"Shape before cleaning: {df_raw.shape}\")\n",
    "\n",
    "# Fit and transform the data\n",
    "# Note: This modifies the column order and converts imputed columns to a NumPy array\n",
    "# We need to reconstruct the DataFrame\n",
    "\n",
    "# Get column names *after* imputation\n",
    "# 1. Columns from numeric imputer\n",
    "# 2. Columns from categorical imputer\n",
    "# 3. 'remainder' columns\n",
    "col_transformer = preprocessing_pipeline.named_steps['impute_features']\n",
    "\n",
    "# Manually get remainder columns\n",
    "imputed_cols = NUMERIC_IMPUTE_COLS + CATEGORICAL_IMPUTE_COLS\n",
    "all_cols_after_parse = preprocessing_pipeline.named_steps['parse_json_strings'].transform(\n",
    "    preprocessing_pipeline.named_steps['clean_list_strings'].transform(\n",
    "        preprocessing_pipeline.named_steps['clean_strings'].transform(\n",
    "            preprocessing_pipeline.named_steps['drop_cols'].transform(df_raw)\n",
    "        )\n",
    "    )\n",
    ").columns\n",
    "remainder_cols = [col for col in all_cols_after_parse if col not in imputed_cols]\n",
    "\n",
    "# Define the final column order\n",
    "final_cols = NUMERIC_IMPUTE_COLS + CATEGORICAL_IMPUTE_COLS + remainder_cols\n",
    "\n",
    "# Apply the pipeline\n",
    "df_cleaned_data = preprocessing_pipeline.fit_transform(df_raw)\n",
    "df_cleaned = pd.DataFrame(df_cleaned_data, columns=final_cols)\n",
    "\n",
    "# Convert numeric columns back to numeric (imputer output is object)\n",
    "for col in NUMERIC_IMPUTE_COLS:\n",
    "    df_cleaned[col] = pd.to_numeric(df_cleaned[col])\n",
    "\n",
    "print(f\"Shape after cleaning: {df_cleaned.shape}\")\n",
    "print(\"\\nCleaned Data Info:\")\n",
    "df_cleaned.info()\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "missing_after = df_cleaned.isnull().sum()\n",
    "print(missing_after[missing_after > 0])\n",
    "\n",
    "# Example of using the outlier remover (e.g., on a training set)\n",
    "# df_train_no_outliers = outlier_remover.fit_transform(df_cleaned)\n",
    "# print(f\"\\nShape after outlier removal: {df_train_no_outliers.shape}\")\n",
    "# print(f\"Removed {len(outlier_remover.outliers)} outlier rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae67b8b",
   "metadata": {},
   "source": [
    "# Key transformation\n",
    "1. Drop Columns (DropColumnTransformer)\n",
    "- t removes ordering_akas, ordering_principal, attributes, and job\n",
    "- These are redundant keys or ordering columns from the original files that are not needed after merging on tconst and nconst. They add noise and increase memory usage\n",
    "\n",
    "2. Standardize Strings (StringCleaner)\n",
    "- Lowercases and strips whitespace from free-text columns like primaryTitle and primaryName\n",
    "- Ensures consistency. \"The Matrix\", \" the matrix \", and \"the matrix\" become identical (\"the matrix\"), which prevents the model from treating them as different categories.\n",
    "\n",
    "3. Clean List-Strings (ListStringCleaner)\n",
    "- Handles comma-separated columns like genres and directors. It turns 'Action, Adventure' into 'action,adventure'\n",
    "- This is crucial for feature engineering.\n",
    "\n",
    "4. Parse JSON-Strings (JSONStringParser)\n",
    "- Specifically targets the characters column, which is often a messy string like '[\"Walter White\", \"Heisenberg\"]'. It parses this into a clean, comma-separated string: 'walter white,heisenberg'\n",
    "- This \"unpacks\" the valuable information (who an actor played) from a format that is otherwise unusable.\n",
    "\n",
    "5. Impute Missing Values (SimpleImputer)\n",
    "* Fills in missing data.\n",
    "- For numeric our strategy is \"median\", so we fill missing values with median \n",
    "- For categorical ones our strategy is \"constant\", we fill each null value with string \"unknown\"\n",
    "\n",
    "!!!CustomOutlierRemoval is included, but separated from the main pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9951508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "#main part\n",
    "url = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/imdb_merged_duckdb.parquet\"\n",
    "\n",
    "#sample \n",
    "#url = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-25_160707_UTC/imdb_sample_100k.parquet\"\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"Loading data into pandas DataFrame using DuckDB...\")\n",
    "\n",
    "# ⭐️ This is the line you need to add:\n",
    "df = con.execute(f\"SELECT * FROM read_parquet('{url}')\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"✅ Data loaded successfully.\")\n",
    "print(df.head())\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68d34b",
   "metadata": {},
   "source": [
    "# Cleaning done with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ada34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ==================================\n",
    "# 1. DEFINE CUSTOM TRANSFORMERS\n",
    "# ==================================\n",
    "\n",
    "class DropColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops specified columns.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns, axis=1, errors='ignore')\n",
    "\n",
    "class StringCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Strips whitespace, lowercases, and converts to string.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[col] = X_transformed[col].astype(str).str.strip().str.lower()\n",
    "        return X_transformed\n",
    "\n",
    "class ListStringCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Cleans comma-separated list-strings (e.g., genres).\"\"\"\n",
    "    def __init__(self, columns, separator=','):\n",
    "        self.columns = columns\n",
    "        self.separator = separator\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[col] = X_transformed[col].fillna('')\n",
    "                X_transformed[col] = X_transformed[col].apply(\n",
    "                    lambda s: self.separator.join(\n",
    "                        [item.strip().lower() for item in str(s).split(self.separator)]\n",
    "                    ) if s else ''\n",
    "                )\n",
    "        return X_transformed\n",
    "\n",
    "class JSONStringParser(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Parses columns containing string representations of JSON lists.\"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def _parse(self, item):\n",
    "        if pd.isna(item):\n",
    "            return ''  # ⭐️⭐️⭐️ THE FIX IS HERE ⭐️⭐️⭐️ (Was '[]', now '\"\"')\n",
    "        try:\n",
    "            parsed_list = json.loads(item)\n",
    "            if isinstance(parsed_list, list):\n",
    "                return ','.join([str(i).strip().lower() for i in parsed_list])\n",
    "            return ''\n",
    "        except (json.JSONDecodeError, TypeError, SyntaxError):\n",
    "            # Handle cases where it's not a valid JSON list (e.g., just \"ActorName\")\n",
    "            return str(item).strip().lower()\n",
    "            \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[col] = X_transformed[col].apply(self._parse)\n",
    "        return X_transformed\n",
    "\n",
    "class CustomOutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Removes rows based on Z-score of specified numeric columns.\"\"\"\n",
    "    def __init__(self, columns, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.columns = columns\n",
    "        self._outliers = None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        valid_cols = [col for col in self.columns if col in X.columns and pd.api.types.is_numeric_dtype(X[col])]\n",
    "        if not valid_cols:\n",
    "            print(\"Warning: No valid numeric columns found for outlier removal.\")\n",
    "            return X_transformed\n",
    "        \n",
    "        # Ensure NaNs are handled before zscore\n",
    "        z_scores = np.abs(stats.zscore(X_transformed[valid_cols], nan_policy='omit'))\n",
    "        \n",
    "        # Fill NaNs in z_scores with 0 (so they aren't considered outliers)\n",
    "        z_scores_filled = np.nan_to_num(z_scores, nan=0)\n",
    "        \n",
    "        mask = (z_scores_filled < self.threshold).all(axis=1)\n",
    "        self._outliers = X_transformed[~mask]\n",
    "        return X_transformed[mask]\n",
    "    @property\n",
    "    def outliers(self):\n",
    "        return self._outliers\n",
    "\n",
    "# ==================================\n",
    "# 2. DEFINE COLUMN GROUPS\n",
    "# ==================================\n",
    "\n",
    "# Columns to remove: Redundant IDs, low-value, or messy\n",
    "DROP_COLS = [\n",
    "    'ordering_akas', \n",
    "    'ordering_principal',\n",
    "    'attributes',\n",
    "    'job'\n",
    "]\n",
    "\n",
    "# Simple strings to clean (lowercase, strip)\n",
    "CLEAN_STRING_COLS = [\n",
    "    'primaryTitle',\n",
    "    'originalTitle',\n",
    "    'title_akas', # Renamed from 'title'\n",
    "    'primaryName'\n",
    "]\n",
    "\n",
    "# Numeric columns to impute (with median)\n",
    "NUMERIC_IMPUTE_COLS = [\n",
    "    'startYear',\n",
    "    'endYear',\n",
    "    'runtimeMinutes',\n",
    "    'averageRating',\n",
    "    'numVotes',\n",
    "    'seasonNumber',\n",
    "    'episodeNumber',\n",
    "    'birthYear',\n",
    "    'deathYear'\n",
    "]\n",
    "\n",
    "# Categorical columns to impute (with 'unknown')\n",
    "CATEGORICAL_IMPUTE_COLS = [\n",
    "    'titleType',\n",
    "    'isAdult',\n",
    "    'region',\n",
    "    'language',\n",
    "    'types',\n",
    "    'category'\n",
    "]\n",
    "\n",
    "# Comma-separated list-strings\n",
    "LIST_STRING_COLS = [\n",
    "    'genres',\n",
    "    'directors',\n",
    "    'writers',\n",
    "    'primaryProfession',\n",
    "    'knownForTitles'\n",
    "]\n",
    "\n",
    "# JSON-like list-strings\n",
    "JSON_STRING_COLS = [\n",
    "    'characters'\n",
    "]\n",
    "\n",
    "# Numeric columns to check for outliers\n",
    "OUTLIER_COLS = [\n",
    "    'runtimeMinutes',\n",
    "    'numVotes',\n",
    "    'averageRating',\n",
    "    'startYear'\n",
    "]\n",
    "\n",
    "# ==================================\n",
    "# 3. BUILD THE CLEANING PIPELINE\n",
    "# ==================================\n",
    "\n",
    "print(\"Building cleaning pipeline...\")\n",
    "\n",
    "# Define imputers\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "\n",
    "# Create the main cleaning pipeline\n",
    "cleaning_pipeline = Pipeline(steps=[\n",
    "    ('drop_cols', DropColumnTransformer(columns=DROP_COLS)),\n",
    "    ('clean_strings', StringCleaner(columns=CLEAN_STRING_COLS)),\n",
    "    ('clean_list_strings', ListStringCleaner(columns=LIST_STRING_COLS)),\n",
    "    ('parse_json_strings', JSONStringParser(columns=JSON_STRING_COLS)),\n",
    "    \n",
    "    ('impute_features', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_impute', numeric_imputer, NUMERIC_IMPUTE_COLS),\n",
    "            ('cat_impute', categorical_imputer, CATEGORICAL_IMPUTE_COLS)\n",
    "        ],\n",
    "        remainder='passthrough' # Keep all other columns\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define the outlier remover separately\n",
    "outlier_remover = CustomOutlierRemover(columns=OUTLIER_COLS, threshold=3)\n",
    "\n",
    "# ==================================\n",
    "# 4. EXECUTE THE PIPELINE\n",
    "# ==================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"1. APPLYING MAIN CLEANING PIPELINE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# We assume 'df' is already loaded in memory\n",
    "print(f\"Shape before cleaning: {df.shape}\")\n",
    "\n",
    "# --- Reconstruct the DataFrame after pipeline ---\n",
    "\n",
    "# Get the list of columns that remain *after* the ColumnTransformer\n",
    "# 1. Get columns *before* the imputer step\n",
    "temp_df = cleaning_pipeline.named_steps['parse_json_strings'].transform(\n",
    "    cleaning_pipeline.named_steps['clean_list_strings'].transform(\n",
    "        cleaning_pipeline.named_steps['clean_strings'].transform(\n",
    "            cleaning_pipeline.named_steps['drop_cols'].transform(df)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Get the 'remainder' columns\n",
    "imputed_cols = NUMERIC_IMPUTE_COLS + CATEGORICAL_IMPUTE_COLS\n",
    "remainder_cols = [col for col in temp_df.columns if col not in imputed_cols]\n",
    "\n",
    "# 3. Define the final column order\n",
    "final_cols = NUMERIC_IMPUTE_COLS + CATEGORICAL_IMPUTE_COLS + remainder_cols\n",
    "\n",
    "# 4. Apply the pipeline\n",
    "df_cleaned_data = cleaning_pipeline.fit_transform(df)\n",
    "df_cleaned = pd.DataFrame(df_cleaned_data, columns=final_cols)\n",
    "\n",
    "# 5. Convert numeric columns back to numeric types\n",
    "for col in NUMERIC_IMPUTE_COLS:\n",
    "    df_cleaned[col] = pd.to_numeric(df_cleaned[col])\n",
    "# Convert isAdult back to numeric/int\n",
    "if 'isAdult' in df_cleaned.columns:\n",
    "    df_cleaned['isAdult'] = pd.to_numeric(df_cleaned['isAdult'])\n",
    "\n",
    "print(f\"Shape after cleaning: {df_cleaned.shape}\")\n",
    "\n",
    "# ==================================\n",
    "# 5. ⭐️ SAVE THE CLEANED FILE FOR COLLEAGUES ⭐️\n",
    "# ==================================\n",
    "OUTPUT_FILE_PARQUET = 'imdb_cleaned_for_colleagues.parquet'\n",
    "print(f\"\\nSaving cleaned data to {OUTPUT_FILE_PARQUET}...\")\n",
    "df_cleaned.to_parquet(OUTPUT_FILE_PARQUET, index=False)\n",
    "print(\"✅ Save complete.\")\n",
    "# ==================================\n",
    "\n",
    "print(\"\\nCleaned Data Info:\")\n",
    "df_cleaned.info(memory_usage='deep')\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"2. APPLYING OUTLIER REMOVAL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "df_no_outliers = outlier_remover.fit_transform(df_cleaned)\n",
    "\n",
    "print(f\"Shape before outlier removal: {df_cleaned.shape}\")\n",
    "print(f\"Shape after outlier removal:  {df_no_outliers.shape}\")\n",
    "print(f\"Removed {len(outlier_remover.outliers)} outlier rows.\")\n",
    "\n",
    "print(\"\\n✅ Full cleaning pipeline complete! File saved for colleagues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f64300",
   "metadata": {},
   "source": [
    "# DUCK DUCK QUERY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install duckdb tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585b9e7",
   "metadata": {},
   "source": [
    "///////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15137d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_imdb_duckdb.py\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "INPUT_URL = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/imdb_merged_duckdb.parquet\"  # can be local path or https URL (if accessible)\n",
    "OUTPUT = \"imdb_merged_cleaned_duckdb.parquet\"\n",
    "DROP_COLS = [\"ordering_x\", \"ordering_y\", \"attributes\", \"job\", \"ordering_akas\", \"ordering_principal\"]\n",
    "CATEGORICAL_FILL = \"unknown\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# register parquet as a view\n",
    "print(\"Registering parquet...\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW imdb_raw AS SELECT * FROM read_parquet('{INPUT_URL}');\")\n",
    "\n",
    "# compute median(s) for numeric imputation (duckdb provides percentile_cont, use .5)\n",
    "medians = {}\n",
    "for col in ['averageRating', 'numVotes', 'seasonNumber', 'episodeNumber']:\n",
    "    try:\n",
    "        res = con.execute(f\"SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY {col}) as med FROM imdb_raw WHERE {col} IS NOT NULL\").fetchone()\n",
    "        med = res[0] if res else 0\n",
    "        medians[col] = med if med is not None else 0\n",
    "    except Exception as e:\n",
    "        medians[col] = 0\n",
    "\n",
    "# compute outlier thresholds on numVotes\n",
    "low_q, high_q = 0, 10**9\n",
    "try:\n",
    "    low, high = con.execute(\"SELECT percentile_cont(0.01) WITHIN GROUP (ORDER BY numVotes) as low, percentile_cont(0.999) WITHIN GROUP (ORDER BY numVotes) as high FROM imdb_raw WHERE numVotes IS NOT NULL\").fetchone()\n",
    "    if low is not None: low_q = low\n",
    "    if high is not None: high_q = high\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Build a cleaned view with SQL transforms\n",
    "# string normalization: lower(trim(...))\n",
    "# list cleans: regexp_replace to normalize spaces around commas\n",
    "# characters: remove [ ] and \" characters via regexp_replace\n",
    "# impute numerics with COALESCE(median)\n",
    "# fill categoricals via COALESCE(col, 'unknown')\n",
    "\n",
    "print(\"Creating cleaned view (this is a streaming SQL operation)...\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW imdb_cleaned AS\n",
    "SELECT\n",
    "    -- drop columns by not selecting them; select explicitly every needed column or use * EXCEPT (duckdb 0.7+ supports EXCEPT)\n",
    "    -- For brevity, use SELECT * EXCEPT(...) if supported; otherwise list columns you want.\n",
    "    -- We'll use * EXCEPT to drop the unwanted cols\n",
    "    * EXCEPT ({', '.join(DROP_COLS)})\n",
    "FROM imdb_raw\n",
    "\"\"\")\n",
    "\n",
    "# Now do a second view that applies column-level transforms:\n",
    "# Build transform SQL dynamically\n",
    "transforms = []\n",
    "# normalize string columns if they exist\n",
    "for col in ['primaryTitle', 'primaryName', 'title', 'region', 'language']:\n",
    "    transforms.append(f\"CASE WHEN {col} IS NULL THEN '{CATEGORICAL_FILL}' ELSE lower(trim({col})) END as {col}\")\n",
    "\n",
    "# list-like columns\n",
    "for col in ['genres','directors','writers']:\n",
    "    transforms.append(f\"CASE WHEN {col} IS NULL THEN '{CATEGORICAL_FILL}' ELSE lower(regexp_replace({col}, '\\\\\\\\s*,\\\\\\\\s*', ',', 'g')) END as {col}\")\n",
    "\n",
    "# characters JSON-clean\n",
    "transforms.append(\"CASE WHEN characters IS NULL THEN '' ELSE lower(regexp_replace(regexp_replace(characters, '\\\\\\\\[|\\\\\\\\]|\\\"', '', 'g'), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g')) END as characters\")\n",
    "\n",
    "# numerics imputation\n",
    "for col, med in medians.items():\n",
    "    transforms.append(f\"COALESCE({col}, {med}) as {col}\")\n",
    "\n",
    "# years coercion (startYear) — try to cast numeric, else null\n",
    "for col in ['startYear','birthYear','deathYear']:\n",
    "    transforms.append(f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN NULL ELSE CAST({col} AS BIGINT) END as {col}\")\n",
    "\n",
    "# Now assemble final select — include all other columns unchanged by using imdb_cleaned.* and then overwrite with transformed ones via FROM ... SELECT\n",
    "transforms_sql = \",\\n    \".join(transforms)\n",
    "\n",
    "final_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW imdb_cleaned_final AS\n",
    "SELECT\n",
    "    -- start with all original columns from imdb_cleaned (note: transformed columns will be overridden below)\n",
    "    imdb_cleaned.*,\n",
    "    {transforms_sql}\n",
    "FROM imdb_cleaned\n",
    "WHERE numVotes BETWEEN {int(low_q)} AND {int(high_q)}\n",
    ";\n",
    "\"\"\"\n",
    "con.execute(final_sql)\n",
    "\n",
    "# Export to parquet\n",
    "print(\"Writing cleaned parquet to:\", OUTPUT)\n",
    "con.execute(f\"COPY (SELECT * FROM imdb_cleaned_final) TO '{OUTPUT}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\")\n",
    "print(\"Done. Output:\", OUTPUT)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7897653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_imdb_duckdb_final_safe.py\n",
    "import duckdb\n",
    "import math\n",
    "\n",
    "# CONFIG\n",
    "INPUT_URL = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/imdb_merged_duckdb.parquet\"  # can be local path or https URL (if accessible)   # local path or http(s) URL if accessible\n",
    "OUTPUT = \"imdb_merged_cleaned_duckdb.parquet\"\n",
    "DROP_COLS = {\"ordering_x\", \"ordering_y\", \"attributes\", \"job\", \"ordering_akas\", \"ordering_principal\"}\n",
    "CATEGORICAL_FILL = \"unknown\"\n",
    "NUMERIC_IMPUTE_COLS = ['averageRating', 'numVotes', 'seasonNumber', 'episodeNumber']\n",
    "TRANSFORM_STRING_COLS = ['primaryTitle', 'primaryName', 'title', 'region', 'language']\n",
    "LIST_COLS = ['genres', 'directors', 'writers']\n",
    "CHAR_COL = 'characters'\n",
    "YEAR_COLS = ['startYear', 'birthYear', 'deathYear']\n",
    "\n",
    "# Helper: classifies DuckDB types simply\n",
    "def is_numeric_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"tinyint\",\"smallint\",\"integer\",\"int\",\"bigint\",\"decimal\",\"numeric\",\"float\",\"double\"])\n",
    "\n",
    "def is_int_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"tinyint\",\"smallint\",\"integer\",\"int\",\"bigint\"])\n",
    "\n",
    "def is_string_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"varchar\",\"text\",\"string\",\"char\"])\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 1) Register parquet as a view\n",
    "con.execute(f\"CREATE OR REPLACE VIEW imdb_raw AS SELECT * FROM read_parquet('{INPUT_URL}');\")\n",
    "\n",
    "# 2) Get actual column names and types\n",
    "cols_info = con.execute(\"DESCRIBE imdb_raw\").fetchall()  # returns list of (name, type, null?)\n",
    "all_cols = [(r[0], r[1]) for r in cols_info]\n",
    "\n",
    "# 3) compute medians for imputeable numeric columns using TRY_CAST\n",
    "medians = {}\n",
    "for col, coltype in all_cols:\n",
    "    if col in NUMERIC_IMPUTE_COLS:\n",
    "        if is_numeric_type(coltype):\n",
    "            try:\n",
    "                res = con.execute(\n",
    "                    f\"SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY TRY_CAST({col} AS DOUBLE)) FROM imdb_raw WHERE {col} IS NOT NULL AND {col} != '\\\\\\\\N'\"\n",
    "                ).fetchone()\n",
    "                med = res[0] if res and res[0] is not None else 0\n",
    "            except Exception:\n",
    "                med = 0\n",
    "        else:\n",
    "            # if the declared type is string, try to compute median by casting values\n",
    "            try:\n",
    "                res = con.execute(\n",
    "                    f\"SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY TRY_CAST({col} AS DOUBLE)) FROM imdb_raw WHERE TRY_CAST({col} AS DOUBLE) IS NOT NULL\"\n",
    "                ).fetchone()\n",
    "                med = res[0] if res and res[0] is not None else 0\n",
    "            except Exception:\n",
    "                med = 0\n",
    "        if med is None or (isinstance(med, float) and (math.isnan(med) or math.isinf(med))):\n",
    "            med = 0\n",
    "        medians[col] = med\n",
    "\n",
    "# 4) compute numVotes quantile bounds for outlier filtering using TRY_CAST\n",
    "low_q, high_q = 0, 10**12\n",
    "if any(c == 'numVotes' for c, _ in all_cols):\n",
    "    try:\n",
    "        low, high = con.execute(\n",
    "            \"SELECT percentile_cont(0.01) WITHIN GROUP (ORDER BY TRY_CAST(numVotes AS DOUBLE)), percentile_cont(0.999) WITHIN GROUP (ORDER BY TRY_CAST(numVotes AS DOUBLE)) FROM imdb_raw WHERE TRY_CAST(numVotes AS DOUBLE) IS NOT NULL\"\n",
    "        ).fetchone()\n",
    "        if low is not None:\n",
    "            low_q = int(max(0, math.floor(low)))\n",
    "        if high is not None:\n",
    "            high_q = int(math.ceil(high))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 5) Build safe SELECT expressions per column (explicit)\n",
    "select_parts = []\n",
    "\n",
    "for col, coltype in all_cols:\n",
    "    if col in DROP_COLS:\n",
    "        continue\n",
    "\n",
    "    # prioritize explicit higher-level transforms\n",
    "    if col in NUMERIC_IMPUTE_COLS:\n",
    "        med = medians.get(col, 0)\n",
    "        # Use TRY_CAST to produce numeric double, fallback to median\n",
    "        select_parts.append(f\"COALESCE(TRY_CAST({col} AS DOUBLE), {med}) AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in YEAR_COLS:\n",
    "        # attempt integer cast, else NULL\n",
    "        # If underlying type is numeric, just TRY_CAST to BIGINT; otherwise TRY_CAST string to BIGINT\n",
    "        select_parts.append(f\"TRY_CAST({col} AS BIGINT) AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in TRANSFORM_STRING_COLS:\n",
    "        # string normalization: replace '\\N' or NULL -> 'unknown', else lower(trim(...))\n",
    "        # But if underlying declared type is numeric, be safe: TRY_CAST -> if numeric then cast to string via CAST(... AS VARCHAR)\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' ELSE lower(trim({col})) END AS {col}\")\n",
    "        else:\n",
    "            # declared numeric but you still want to present as string: convert safely\n",
    "            select_parts.append(f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(trim(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR))) ELSE '{CATEGORICAL_FILL}' END AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in LIST_COLS:\n",
    "        # normalize commas and lowercase; treat '\\N' as unknown\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' ELSE lower(regexp_replace(regexp_replace({col}, '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) END AS {col}\"\n",
    "            )\n",
    "        else:\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(regexp_replace(regexp_replace(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) ELSE '{CATEGORICAL_FILL}' END AS {col}\"\n",
    "            )\n",
    "        continue\n",
    "\n",
    "    if col == CHAR_COL:\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '' ELSE lower(regexp_replace(regexp_replace(regexp_replace({col}, '\\\\\\\\[|\\\\\\\\]|\\\"', '', 'g'), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) END AS {col}\"\n",
    "            )\n",
    "        else:\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(regexp_replace(regexp_replace(regexp_replace(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR), '\\\\\\\\[|\\\\\\\\]|\\\"', '', 'g'), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) ELSE '' END AS {col}\"\n",
    "            )\n",
    "        continue\n",
    "\n",
    "    # Default handling based on declared type:\n",
    "    if is_numeric_type(coltype):\n",
    "        # Ensure numeric output: TRY_CAST to DOUBLE (NULL if not castable)\n",
    "        select_parts.append(f\"TRY_CAST({col} AS DOUBLE) AS {col}\")\n",
    "    elif is_string_type(coltype):\n",
    "        # Replace literal '\\N' with NULL; keep string as-is (optionally trim/lower if you want)\n",
    "        select_parts.append(f\"CASE WHEN {col} = '\\\\\\\\N' THEN NULL ELSE {col} END AS {col}\")\n",
    "    else:\n",
    "        # fallback: try to TRY_CAST to DOUBLE, else pass through with '\\N' -> NULL guard\n",
    "        select_parts.append(f\"CASE WHEN {col} = '\\\\\\\\N' THEN NULL WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN TRY_CAST({col} AS DOUBLE) ELSE {col} END AS {col}\")\n",
    "\n",
    "# 6) Assemble final SQL\n",
    "final_select_sql = \",\\n    \".join(select_parts)\n",
    "where_clause = \"1=1\"\n",
    "if any(c == 'numVotes' for c, _ in all_cols):\n",
    "    # use TRY_CAST in where to avoid conversion errors\n",
    "    where_clause = f\"TRY_CAST(numVotes AS DOUBLE) BETWEEN {low_q} AND {high_q}\"\n",
    "\n",
    "create_view_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW imdb_cleaned_final AS\n",
    "SELECT\n",
    "    {final_select_sql}\n",
    "FROM imdb_raw\n",
    "WHERE {where_clause}\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# Optional: print a snippet for debugging\n",
    "# print(create_view_sql[:2000])\n",
    "\n",
    "# 7) Execute view creation & export\n",
    "con.execute(create_view_sql)\n",
    "\n",
    "print(f\"Writing cleaned parquet to: {OUTPUT}\")\n",
    "# use COPY which streams\n",
    "con.execute(f\"COPY (SELECT * FROM imdb_cleaned_final) TO '{OUTPUT}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\")\n",
    "print(\"Done. Output:\", OUTPUT)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a76387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# ==================================\n",
    "# 0. CONFIGURATION\n",
    "# ==================================\n",
    "\n",
    "# ⭐️ ASSUMPTION: This is the file you created with the *filtered merge* script\n",
    "INPUT_FILE = \"imdb_merged_duckdb_FILTERED.parquet\" \n",
    "\n",
    "# This is the new, clean file we will create\n",
    "OUTPUT_FILE = \"imdb_cleaned_for_colleagues.parquet\"\n",
    "\n",
    "# ==================================\n",
    "# PART 1: DUCKDB CLEANING PIPELINE\n",
    "# ==================================\n",
    "\n",
    "print(f\"🦆 Starting DuckDB cleaning pipeline on '{INPUT_FILE}'...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to an in-memory database\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# --- Register the raw Parquet file as a virtual table ---\n",
    "try:\n",
    "    con.execute(f\"CREATE VIEW raw_data AS SELECT * FROM read_parquet('{INPUT_FILE}');\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR: Could not read '{INPUT_FILE}'.\")\n",
    "    print(\"   Did you run the filtered merge script first? Is the file name correct?\")\n",
    "    print(f\"   Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"1. Building DuckDB cleaning query...\")\n",
    "\n",
    "# --- Define Column Groups (for SQL) ---\n",
    "# These lists help build the query\n",
    "# Note: DROP_COLS are handled by not SELECT-ing them\n",
    "\n",
    "CLEAN_STRING_COLS = ['primaryTitle', 'originalTitle', 'title_akas', 'primaryName']\n",
    "NUMERIC_IMPUTE_COLS = ['startYear', 'endYear', 'runtimeMinutes', 'averageRating', 'numVotes', 'seasonNumber', 'episodeNumber', 'birthYear', 'deathYear']\n",
    "CATEGORICAL_IMPUTE_COLS = ['titleType', 'isAdult', 'region', 'language', 'types', 'category']\n",
    "LIST_STRING_COLS = ['genres', 'directors', 'writers', 'primaryProfession', 'knownForTitles']\n",
    "JSON_STRING_COLS = ['characters']\n",
    "\n",
    "# --- Build the Main Cleaning Query ---\n",
    "\n",
    "# 1. Create the CTE for medians (for numeric imputation)\n",
    "median_calculations = []\n",
    "for col in NUMERIC_IMPUTE_COLS:\n",
    "    median_calculations.append(f\"median({col}) AS med_{col}\")\n",
    "median_cte = f\"WITH NumericMedians AS (SELECT {', '.join(median_calculations)} FROM raw_data)\"\n",
    "\n",
    "# 2. Create the main SELECT statements\n",
    "select_statements = [\n",
    "    # --- Keep Key IDs ---\n",
    "    \"tconst\",\n",
    "    \"nconst\",\n",
    "    \"parentTconst\", # From episode table\n",
    "    \"isOriginalTitle\", # From akas table\n",
    "]\n",
    "\n",
    "# --- String Cleaning (LOWER, TRIM) ---\n",
    "for col in CLEAN_STRING_COLS:\n",
    "    select_statements.append(f\"LOWER(TRIM({col})) AS {col}\")\n",
    "\n",
    "# --- List-String Cleaning (LOWER, TRIM, COALESCE) ---\n",
    "for col in LIST_STRING_COLS:\n",
    "    select_statements.append(f\"LOWER(TRIM(COALESCE({col}, ''))) AS {col}\")\n",
    "\n",
    "# --- JSON-String Parsing (REGEXP_REPLACE) ---\n",
    "for col in JSON_STRING_COLS:\n",
    "    # This strips [\" and \"] characters, then trims and lowers\n",
    "    select_statements.append(f\"LOWER(TRIM(regexp_replace(COALESCE({col}, ''), '[\\\"\\\\[\\\\]]', '', 'g'))) AS {col}\")\n",
    "\n",
    "# --- Categorical Imputation (COALESCE 'unknown') ---\n",
    "for col in CATEGORICAL_IMPUTE_COLS:\n",
    "    select_statements.append(f\"COALESCE(LOWER(TRIM({col})), 'unknown') AS {col}\")\n",
    "    \n",
    "# --- Numeric Imputation (COALESCE with median) ---\n",
    "for col in NUMERIC_IMPUTE_COLS:\n",
    "    select_statements.append(f\"COALESCE({col}, (SELECT med_{col} FROM NumericMedians)) AS {col}\")\n",
    "\n",
    "# --- Combine all parts into the final query ---\n",
    "final_cleaning_query = f\"\"\"\n",
    "{median_cte}\n",
    "SELECT\n",
    "    {', '.join(select_statements)}\n",
    "FROM raw_data\n",
    "\"\"\"\n",
    "\n",
    "# 3. Execute the query and save the result\n",
    "print(f\"2. Running query and saving to '{OUTPUT_FILE}'...\")\n",
    "try:\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        {final_cleaning_query}\n",
    "    ) \n",
    "    TO '{OUTPUT_FILE}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR: The DuckDB cleaning query failed.\")\n",
    "    print(f\"   Details: {e}\")\n",
    "    con.close()\n",
    "    exit()\n",
    "\n",
    "con.close()\n",
    "elapsed = (time.time() - start_time)\n",
    "print(f\"✅ DuckDB cleaning complete. File saved. (Elapsed: {elapsed:.2f}s)\")\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# PART 2: PANDAS OUTLIER REMOVAL\n",
    "# ==================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" PANDAS OUTLIER REMOVAL STAGE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- Load the NEW, CLEAN file we just saved ---\n",
    "print(f\"Loading '{OUTPUT_FILE}' into pandas...\")\n",
    "try:\n",
    "    df_cleaned = pd.read_parquet(OUTPUT_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Cannot find '{OUTPUT_FILE}'. The cleaning script must have failed.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Cleaned Data Info:\")\n",
    "df_cleaned.info(memory_usage='deep')\n",
    "\n",
    "# --- Define the Outlier Remover Class (Unchanged) ---\n",
    "class CustomOutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Removes rows based on Z-score of specified numeric columns.\"\"\"\n",
    "    def __init__(self, columns, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.columns = columns\n",
    "        self._outliers = None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        valid_cols = [col for col in self.columns if col in X.columns and pd.api.types.is_numeric_dtype(X[col])]\n",
    "        if not valid_cols:\n",
    "            print(\"Warning: No valid numeric columns found for outlier removal.\")\n",
    "            return X_transformed\n",
    "        z_scores = np.abs(stats.zscore(X_transformed[valid_cols], nan_policy='omit'))\n",
    "        z_scores_filled = np.nan_to_num(z_scores, nan=0)\n",
    "        mask = (z_scores_filled < self.threshold).all(axis=1)\n",
    "        self._outliers = X_transformed[~mask]\n",
    "        return X_transformed[mask]\n",
    "    @property\n",
    "    def outliers(self):\n",
    "        return self._outliers\n",
    "\n",
    "# --- Define Outlier Columns (Unchanged) ---\n",
    "OUTLIER_COLS = [\n",
    "    'runtimeMinutes',\n",
    "    'numVotes',\n",
    "    'averageRating',\n",
    "    'startYear'\n",
    "]\n",
    "\n",
    "# --- Run the Outlier Remover ---\n",
    "print(\"\\nApplying outlier removal...\")\n",
    "outlier_remover = CustomOutlierRemover(columns=OUTLIER_COLS, threshold=3)\n",
    "df_no_outliers = outlier_remover.fit_transform(df_cleaned)\n",
    "\n",
    "print(f\"Shape before outlier removal: {df_cleaned.shape}\")\n",
    "print(f\"Shape after outlier removal:  {df_no_outliers.shape}\")\n",
    "print(f\"Removed {len(outlier_remover.outliers)} outlier rows.\")\n",
    "\n",
    "print(\"\\n✅ Full pipeline complete! File saved for colleagues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c900dcf",
   "metadata": {},
   "source": [
    "# Filtered DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "#main part\n",
    "url = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/imdb_merged_duckdb.parquet\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "print(\"Loading data into pandas DataFrame using DuckDB...\")\n",
    "\n",
    "# ⭐️ This is the line you need to add:\n",
    "df = con.execute(f\"SELECT * FROM read_parquet('{url}')\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"✅ Data loaded successfully.\")\n",
    "print(df.head())\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2f964",
   "metadata": {},
   "source": [
    "# We did it\n",
    "I do it locally, with the usage of the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79144113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_imdb_duckdb_final_safe.py\n",
    "import duckdb\n",
    "import math\n",
    "\n",
    "# CONFIG\n",
    "INPUT_URL = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/imdb_merged_duckdb.parquet\"  # can be local path or https URL (if accessible)   # local path or http(s) URL if accessible\n",
    "OUTPUT = \"imdb_merged_cleaned_duckdb.parquet\"\n",
    "DROP_COLS = {\"ordering_x\", \"ordering_y\", \"attributes\", \"job\", \"ordering_akas\", \"ordering_principal\"}\n",
    "CATEGORICAL_FILL = \"unknown\"\n",
    "NUMERIC_IMPUTE_COLS = ['averageRating', 'numVotes', 'seasonNumber', 'episodeNumber']\n",
    "TRANSFORM_STRING_COLS = ['primaryTitle', 'primaryName', 'title', 'region', 'language']\n",
    "LIST_COLS = ['genres', 'directors', 'writers']\n",
    "CHAR_COL = 'characters'\n",
    "YEAR_COLS = ['startYear', 'birthYear', 'deathYear']\n",
    "\n",
    "# Helper: classifies DuckDB types simply\n",
    "def is_numeric_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"tinyint\",\"smallint\",\"integer\",\"int\",\"bigint\",\"decimal\",\"numeric\",\"float\",\"double\"])\n",
    "\n",
    "def is_int_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"tinyint\",\"smallint\",\"integer\",\"int\",\"bigint\"])\n",
    "\n",
    "def is_string_type(duck_type: str):\n",
    "    t = duck_type.lower()\n",
    "    return any(x in t for x in [\"varchar\",\"text\",\"string\",\"char\"])\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 1) Register parquet as a view\n",
    "con.execute(f\"CREATE OR REPLACE VIEW imdb_raw AS SELECT * FROM read_parquet('{INPUT_URL}');\")\n",
    "\n",
    "# 2) Get actual column names and types\n",
    "cols_info = con.execute(\"DESCRIBE imdb_raw\").fetchall()  # returns list of (name, type, null?)\n",
    "all_cols = [(r[0], r[1]) for r in cols_info]\n",
    "\n",
    "# 3) compute medians for imputeable numeric columns using TRY_CAST\n",
    "medians = {}\n",
    "for col, coltype in all_cols:\n",
    "    if col in NUMERIC_IMPUTE_COLS:\n",
    "        if is_numeric_type(coltype):\n",
    "            try:\n",
    "                res = con.execute(\n",
    "                    f\"SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY TRY_CAST({col} AS DOUBLE)) FROM imdb_raw WHERE {col} IS NOT NULL AND {col} != '\\\\\\\\N'\"\n",
    "                ).fetchone()\n",
    "                med = res[0] if res and res[0] is not None else 0\n",
    "            except Exception:\n",
    "                med = 0\n",
    "        else:\n",
    "            # if the declared type is string, try to compute median by casting values\n",
    "            try:\n",
    "                res = con.execute(\n",
    "                    f\"SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY TRY_CAST({col} AS DOUBLE)) FROM imdb_raw WHERE TRY_CAST({col} AS DOUBLE) IS NOT NULL\"\n",
    "                ).fetchone()\n",
    "                med = res[0] if res and res[0] is not None else 0\n",
    "            except Exception:\n",
    "                med = 0\n",
    "        if med is None or (isinstance(med, float) and (math.isnan(med) or math.isinf(med))):\n",
    "            med = 0\n",
    "        medians[col] = med\n",
    "\n",
    "# 4) compute numVotes quantile bounds for outlier filtering using TRY_CAST\n",
    "low_q, high_q = 0, 10**12\n",
    "if any(c == 'numVotes' for c, _ in all_cols):\n",
    "    try:\n",
    "        low, high = con.execute(\n",
    "            \"SELECT percentile_cont(0.01) WITHIN GROUP (ORDER BY TRY_CAST(numVotes AS DOUBLE)), percentile_cont(0.999) WITHIN GROUP (ORDER BY TRY_CAST(numVotes AS DOUBLE)) FROM imdb_raw WHERE TRY_CAST(numVotes AS DOUBLE) IS NOT NULL\"\n",
    "        ).fetchone()\n",
    "        if low is not None:\n",
    "            low_q = int(max(0, math.floor(low)))\n",
    "        if high is not None:\n",
    "            high_q = int(math.ceil(high))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 5) Build safe SELECT expressions per column (explicit)\n",
    "select_parts = []\n",
    "\n",
    "for col, coltype in all_cols:\n",
    "    if col in DROP_COLS:\n",
    "        continue\n",
    "\n",
    "    # prioritize explicit higher-level transforms\n",
    "    if col in NUMERIC_IMPUTE_COLS:\n",
    "        med = medians.get(col, 0)\n",
    "        # Use TRY_CAST to produce numeric double, fallback to median\n",
    "        select_parts.append(f\"COALESCE(TRY_CAST({col} AS DOUBLE), {med}) AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in YEAR_COLS:\n",
    "        # attempt integer cast, else NULL\n",
    "        # If underlying type is numeric, just TRY_CAST to BIGINT; otherwise TRY_CAST string to BIGINT\n",
    "        select_parts.append(f\"TRY_CAST({col} AS BIGINT) AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in TRANSFORM_STRING_COLS:\n",
    "        # string normalization: replace '\\N' or NULL -> 'unknown', else lower(trim(...))\n",
    "        # But if underlying declared type is numeric, be safe: TRY_CAST -> if numeric then cast to string via CAST(... AS VARCHAR)\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' ELSE lower(trim({col})) END AS {col}\")\n",
    "        else:\n",
    "            # declared numeric but you still want to present as string: convert safely\n",
    "            select_parts.append(f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(trim(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR))) ELSE '{CATEGORICAL_FILL}' END AS {col}\")\n",
    "        continue\n",
    "\n",
    "    if col in LIST_COLS:\n",
    "        # normalize commas and lowercase; treat '\\N' as unknown\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' ELSE lower(regexp_replace(regexp_replace({col}, '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) END AS {col}\"\n",
    "            )\n",
    "        else:\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '{CATEGORICAL_FILL}' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(regexp_replace(regexp_replace(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) ELSE '{CATEGORICAL_FILL}' END AS {col}\"\n",
    "            )\n",
    "        continue\n",
    "\n",
    "    if col == CHAR_COL:\n",
    "        if is_string_type(coltype):\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '' ELSE lower(regexp_replace(regexp_replace(regexp_replace({col}, '\\\\\\\\[|\\\\\\\\]|\\\"', '', 'g'), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) END AS {col}\"\n",
    "            )\n",
    "        else:\n",
    "            select_parts.append(\n",
    "                f\"CASE WHEN {col} IS NULL OR {col} = '\\\\\\\\N' THEN '' WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN lower(regexp_replace(regexp_replace(regexp_replace(CAST(TRY_CAST({col} AS DOUBLE) AS VARCHAR), '\\\\\\\\[|\\\\\\\\]|\\\"', '', 'g'), '\\\\\\\\s*,\\\\\\\\s*', ',', 'g'), '^,+|,+$', '', 'g')) ELSE '' END AS {col}\"\n",
    "            )\n",
    "        continue\n",
    "\n",
    "    # Default handling based on declared type:\n",
    "    if is_numeric_type(coltype):\n",
    "        # Ensure numeric output: TRY_CAST to DOUBLE (NULL if not castable)\n",
    "        select_parts.append(f\"TRY_CAST({col} AS DOUBLE) AS {col}\")\n",
    "    elif is_string_type(coltype):\n",
    "        # Replace literal '\\N' with NULL; keep string as-is (optionally trim/lower if you want)\n",
    "        select_parts.append(f\"CASE WHEN {col} = '\\\\\\\\N' THEN NULL ELSE {col} END AS {col}\")\n",
    "    else:\n",
    "        # fallback: try to TRY_CAST to DOUBLE, else pass through with '\\N' -> NULL guard\n",
    "        select_parts.append(f\"CASE WHEN {col} = '\\\\\\\\N' THEN NULL WHEN TRY_CAST({col} AS DOUBLE) IS NOT NULL THEN TRY_CAST({col} AS DOUBLE) ELSE {col} END AS {col}\")\n",
    "\n",
    "# 6) Assemble final SQL\n",
    "final_select_sql = \",\\n    \".join(select_parts)\n",
    "where_clause = \"1=1\"\n",
    "if any(c == 'numVotes' for c, _ in all_cols):\n",
    "    # use TRY_CAST in where to avoid conversion errors\n",
    "    where_clause = f\"TRY_CAST(numVotes AS DOUBLE) BETWEEN {low_q} AND {high_q}\"\n",
    "\n",
    "create_view_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW imdb_cleaned_final AS\n",
    "SELECT\n",
    "    {final_select_sql}\n",
    "FROM imdb_raw\n",
    "WHERE {where_clause}\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# Optional: print a snippet for debugging\n",
    "# print(create_view_sql[:2000])\n",
    "\n",
    "# 7) Execute view creation & export\n",
    "con.execute(create_view_sql)\n",
    "\n",
    "print(f\"Writing cleaned parquet to: {OUTPUT}\")\n",
    "# use COPY which streams\n",
    "con.execute(f\"COPY (SELECT * FROM imdb_cleaned_final) TO '{OUTPUT}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\")\n",
    "print(\"Done. Output:\", OUTPUT)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# The URL to your cleaned file\n",
    "url = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-26_112727_UTC/imdb_merged_cleaned_duckdb.parquet\"\n",
    "\n",
    "# Connect to an in-memory database\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# 1. Install and load the httpfs extension\n",
    "# This is required to read files from URLs\n",
    "print(\"Loading httpfs extension...\")\n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "\n",
    "# 2. Create a VIEW (this is instant and uses no memory)\n",
    "# This just tells DuckDB where the file is. It doesn't download it.\n",
    "print(f\"Registering URL as 'imdb_cleaned' view...\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW imdb_cleaned AS SELECT * FROM '{url}'\")\n",
    "\n",
    "print(\"\\n✅ Done! The view 'imdb_cleaned' is ready to be queried.\")\n",
    "\n",
    "# ==========================================================\n",
    "#  NOW YOU CAN QUERY IT SAFELY:\n",
    "# ==========================================================\n",
    "\n",
    "# Example 1: Get 10 rows to see the columns\n",
    "print(\"\\n--- Example 1: Grabbing 10 rows ---\")\n",
    "df_sample = con.sql(\"SELECT * FROM imdb_cleaned LIMIT 10\").df()\n",
    "print(df_sample)\n",
    "\n",
    "\n",
    "# Example 2: Run an aggregation\n",
    "# DuckDB does the heavy work, you just get the small result.\n",
    "print(\"\\n--- Example 2: Running a safe aggregation ---\")\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    titleType, \n",
    "    COUNT(*) as total_rows\n",
    "FROM imdb_cleaned \n",
    "GROUP BY titleType\n",
    "\"\"\"\n",
    "df_agg = con.sql(query).df()\n",
    "\n",
    "print(df_agg)\n",
    "\n",
    "# You can keep using 'con' to run any query you want on the 'imdb_cleaned' view\n",
    "# con.close() # Close it when you're all done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5852242",
   "metadata": {},
   "source": [
    "# Checking how our cleaned data looks like on a sample cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a2fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tconst  titleType          primaryTitle       originalTitle  isAdult  \\\n",
      "0   tt0207871   tvSeries             buccaneer           Buccaneer      0.0   \n",
      "1   tt0118694      movie  in the mood for love    Fa yeung nin wah      0.0   \n",
      "2  tt13586826  tvEpisode    allumer le camping  Allumer le Camping      0.0   \n",
      "3   tt2226407      movie         the landlords     Padroni di casa      0.0   \n",
      "4  tt28636869  tvEpisode          episode #2.4        Episode #2.4      0.0   \n",
      "\n",
      "   startYear endYear  runtimeMinutes                  genres  averageRating  \\\n",
      "0     1980.0    None            50.0         adventure,drama            6.8   \n",
      "1     2000.0    None            98.0           drama,romance            8.0   \n",
      "2     2020.0    None            45.0                  comedy            5.4   \n",
      "3     2012.0    None            90.0                   drama            6.2   \n",
      "4     2023.0    None             NaN  action,adventure,drama            8.0   \n",
      "\n",
      "   ...  ordering_1     nconst  category                characters   nconst_1  \\\n",
      "0  ...        10.0  nm0204096     actor              [accountant]  nm0204096   \n",
      "1  ...         1.0  nm0001041   actress  [su li-zhen - mrs. chan]  nm0001041   \n",
      "2  ...         5.0  nm7032070   actress            [audrey dukor]  nm7032070   \n",
      "3  ...        19.0  nm1600173  composer                            nm1600173   \n",
      "4  ...         1.0  nm0453640     actor              [jin ho-gae]  nm0453640   \n",
      "\n",
      "        primaryName birthYear  deathYear                   primaryProfession  \\\n",
      "0   geoffrey davion    1940.0     1996.0                               actor   \n",
      "1     maggie cheung    1964.0        NaN  actress,soundtrack,archive_footage   \n",
      "2           candiie       NaN        NaN                      actress,writer   \n",
      "3  cesare cremonini    1980.0        NaN               composer,actor,writer   \n",
      "4       kim rae-won    1981.0        NaN                               actor   \n",
      "\n",
      "                              knownForTitles  \n",
      "0    tt0087749,tt0072566,tt0090852,tt0065290  \n",
      "1    tt0118694,tt0299977,tt0101258,tt0117905  \n",
      "2  tt0832440,tt15710992,tt11502554,tt8304664  \n",
      "3    tt2226407,tt2039331,tt0813681,tt0306458  \n",
      "4    tt4329922,tt0395140,tt1491379,tt0266855  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20968 entries, 0 to 20967\n",
      "Data columns (total 34 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   tconst             20968 non-null  object \n",
      " 1   titleType          20968 non-null  object \n",
      " 2   primaryTitle       20968 non-null  object \n",
      " 3   originalTitle      20968 non-null  object \n",
      " 4   isAdult            20968 non-null  float64\n",
      " 5   startYear          20966 non-null  float64\n",
      " 6   endYear            1426 non-null   object \n",
      " 7   runtimeMinutes     17571 non-null  float64\n",
      " 8   genres             20968 non-null  object \n",
      " 9   averageRating      20968 non-null  float64\n",
      " 10  numVotes           20968 non-null  float64\n",
      " 11  directors          20968 non-null  object \n",
      " 12  writers            20968 non-null  object \n",
      " 13  parentTconst       6058 non-null   object \n",
      " 14  seasonNumber       20968 non-null  float64\n",
      " 15  episodeNumber      20968 non-null  float64\n",
      " 16  titleId            20968 non-null  object \n",
      " 17  ordering           20968 non-null  float64\n",
      " 18  title              20968 non-null  object \n",
      " 19  region             20968 non-null  object \n",
      " 20  language           20968 non-null  object \n",
      " 21  types              15601 non-null  object \n",
      " 22  isOriginalTitle    20968 non-null  float64\n",
      " 23  tconst_1           20956 non-null  object \n",
      " 24  ordering_1         20956 non-null  float64\n",
      " 25  nconst             20956 non-null  object \n",
      " 26  category           20956 non-null  object \n",
      " 27  characters         20968 non-null  object \n",
      " 28  nconst_1           20954 non-null  object \n",
      " 29  primaryName        20968 non-null  object \n",
      " 30  birthYear          11762 non-null  float64\n",
      " 31  deathYear          3882 non-null   float64\n",
      " 32  primaryProfession  20579 non-null  object \n",
      " 33  knownForTitles     20936 non-null  object \n",
      "dtypes: float64(12), object(22)\n",
      "memory usage: 5.4+ MB\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-26_114928_UTC/imdb_merged_cleaned_duckdb.parquet\"\n",
    "df = pd.read_parquet(url, engine=\"pyarrow\")\n",
    "\n",
    "print(df.head())\n",
    "print('\\n', df.info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
