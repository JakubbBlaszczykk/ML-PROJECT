{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af00e3d9",
   "metadata": {},
   "source": [
    "# DATA LOADING + DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d71ce",
   "metadata": {},
   "source": [
    "# Brief Summary of Our DuckDB Merging Process\n",
    "\n",
    "## 1. GOAL\n",
    "\n",
    "Merge all seven remote IMDb .tsv.gz datasets into one master Parquet file. The final table should contain all information for each title, joined across all relevant tables.\n",
    "\n",
    "## 2. CHALLENGES\n",
    "\n",
    "* Datasets are remote (hosted on Azure Blob Storage).\n",
    "\n",
    "* Files are large and numerous; loading them all into memory (e.g., with Pandas) would crash the kernel.\n",
    "\n",
    "* We need an efficient way to perform SQL-style joins without manual chunking.\n",
    "\n",
    "## 3. APPROACH (DuckDB Virtual Tables & Staged Joins)\n",
    "\n",
    "We use DuckDB to handle all the heavy lifting directly, without loading data into Python.\n",
    "\n",
    "1. Connect: Create an in-memory DuckDB database (:memory:).\n",
    "\n",
    "2. Register Files as VIEWs: Use CREATE OR REPLACE VIEW ... AS SELECT * FROM read_csv(...) for all seven .tsv.gz files.\n",
    "\n",
    "   * DuckDB reads directly from the HTTPS URLs.\n",
    "\n",
    "   * It decompresses the gzip files on the fly.\n",
    "\n",
    "   * It handles TSV format (delim='\\\\t') and null values (nullstr='\\\\\\\\N').\n",
    "\n",
    "   * VIEWs are lazy and cost no memory; they are just stored queries.\n",
    "\n",
    "3. Staged Joins: Instead of one massive join, we build the final table in steps, materializing each result into a new TABLE. This controls memory usage.\n",
    "\n",
    "   * merged_core: title.basics + ratings + crew + episode\n",
    "\n",
    "   * merged_with_akas: merged_core + title.akas\n",
    "\n",
    "   * merged_with_principals: merged_with_akas + title.principals\n",
    "\n",
    "   * imdb_final: merged_with_principals + name.basics\n",
    "\n",
    "4. Export to Parquet: Use the COPY command to stream the final imdb_final table directly to a compressed Parquet file (imdb_merged_duckdb.parquet).\n",
    "\n",
    "## 4. HOW MERGING WORKS (SQL)\n",
    "\n",
    "* We use LEFT JOIN ... USING (tconst) or LEFT JOIN ... ON (mp.nconst = n.nconst).\n",
    "\n",
    "* LEFT JOIN: This is the equivalent of Pandas how=\"left\". It keeps every row from the \"left\" table (e.g., merged_with_akas) and joins any matching information from the \"right\" table (e.g., title.principals). If there's no match, the columns from the right table are filled with NULL.\n",
    "\n",
    "* USING (tconst): This is a convenient shortcut for ON table_A.tconst = table_B.tconst when the key column has the same name in both tables.\n",
    "\n",
    "## 5. ADVANTAGES OF THE DUCKDB APPROACH\n",
    "\n",
    "* Dedicated Compute: Running this pipeline on a capable Azure ML compute instance (like computeAlan) provides the necessary CPU and memory resources for DuckDB to perform these large-scale operations efficiently.\n",
    "\n",
    "* Extremely Memory-Efficient: At no point is the entire dataset loaded into Python's memory. DuckDB streams data from the remote files, processes it, and streams the output to disk.\n",
    "\n",
    "* Very Fast: DuckDB's vectorized query engine is highly optimized for this kind of work (joins, filtering, writing Parquet).\n",
    "\n",
    "* No Manual Chunking: We don't need to write complex Python loops to manage chunks; DuckDB handles the streaming internally.\n",
    "\n",
    "* Simple & Declarative: The logic is expressed in clean, standard SQL.\n",
    "\n",
    "## 6. NOTES AFTER MERGE\n",
    "\n",
    "* Multiple Rows per Title (Expected!): The final Parquet file will have multiple rows for a single tconst (title). This is normal and correct, as a title can have:\n",
    "\n",
    "  * One row per actor/crew member (from title.principals).\n",
    "\n",
    "  * One row per alternate title/region (from title.akas).\n",
    "\n",
    "* Next Step (Cleaning): This merged file is now the perfect input for a cleaning pipeline (like the Dask script). During cleaning, you can:\n",
    "\n",
    "  * Handle any NULL values that resulted from the joins.\n",
    "\n",
    "  * Normalize genres, types, etc.\n",
    "\n",
    "  * Potentially aggregate data (e.g., group by tconst and create a list of actors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "#  Azure URLs\n",
    "# =======================\n",
    "URL_NAME_BASICS    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104122_UTC/name.basics.tsv.gz\"\n",
    "URL_TITLE_AKAS     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104546_UTC/title.akas.tsv.gz\"\n",
    "URL_TITLE_BASICS   = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104810_UTC/title.basics.tsv.gz\"\n",
    "URL_TITLE_CREW     = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_104937_UTC/title.crew.tsv.gz\"\n",
    "URL_TITLE_EPISODE  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105103_UTC/title.episode.tsv.gz\"\n",
    "URL_TITLE_PRINC    = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105225_UTC/title.principals.tsv.gz\"\n",
    "URL_TITLE_RATINGS  = \"https://workspace4824871889.blob.core.windows.net/azureml-blobstore-84f516da-0fe5-4f33-8f3c-f18ec8e2b4f7/UI/2025-10-22_105430_UTC/title.ratings.tsv.gz\"\n",
    "\n",
    "# =======================\n",
    "#  Start DuckDB connection\n",
    "# =======================\n",
    "# Connect to an in-memory database or specify a file: database='imdb_merge.duckdb'\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "start_time = time.time()\n",
    "print(\" Starting IMDb data merge using DuckDB...\\n\")\n",
    "\n",
    "# =======================\n",
    "#  Stage progress tracker\n",
    "# =======================\n",
    "stages = [\n",
    "    \"Registering IMDb files\",\n",
    "    \"Joining basics + ratings + crew + episode\",\n",
    "    \"Joining akas\",\n",
    "    \"Joining principals\",\n",
    "    \"Joining name.basics\",\n",
    "    \"Exporting to Parquet\"\n",
    "]\n",
    "progress = tqdm(total=len(stages), desc=\"Progress\", ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
    "\n",
    "# =======================\n",
    "#  Register TSV.GZ files as virtual tables\n",
    "# =======================\n",
    "# DuckDB can read directly from HTTPS URLs and handle compressed files.\n",
    "# 'auto_detect=True' helps with schema, but we specify key params.\n",
    "base_read_csv = \"SELECT * FROM read_csv('{}', delim='\\\\t', nullstr='\\\\\\\\N', header=True, compression='gzip', auto_detect=True, parallel=True)\"\n",
    "\n",
    "con.execute(f\"CREATE OR REPLACE VIEW name_basics AS {base_read_csv.format(URL_NAME_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_basics AS {base_read_csv.format(URL_TITLE_BASICS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_ratings AS {base_read_csv.format(URL_TITLE_RATINGS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_crew AS {base_read_csv.format(URL_TITLE_CREW)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_episode AS {base_read_csv.format(URL_TITLE_EPISODE)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_akas AS {base_read_csv.format(URL_TITLE_AKAS)};\")\n",
    "con.execute(f\"CREATE OR REPLACE VIEW title_principals AS {base_read_csv.format(URL_TITLE_PRINC)};\")\n",
    "\n",
    "progress.update(1)\n",
    "progress.set_description(stages[1])\n",
    "\n",
    "# =======================\n",
    "#  Perform joins step by step\n",
    "# =======================\n",
    "# This step-by-step materialization helps manage memory\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_core AS\n",
    "SELECT *\n",
    "FROM title_basics b\n",
    "LEFT JOIN title_ratings r USING (tconst)\n",
    "LEFT JOIN title_crew c USING (tconst)\n",
    "LEFT JOIN title_episode e USING (tconst);\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[2])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_with_akas AS\n",
    "SELECT *\n",
    "FROM merged_core mc\n",
    "LEFT JOIN title_akas a ON mc.tconst = a.titleId;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[3])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE merged_with_principals AS\n",
    "SELECT *\n",
    "FROM merged_with_akas ma\n",
    "LEFT JOIN title_principals p ON ma.tconst = p.tconst;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[4])\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE imdb_final AS\n",
    "SELECT *\n",
    "FROM merged_with_principals mp\n",
    "LEFT JOIN name_basics n ON mp.nconst = n.nconst;\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.set_description(stages[5])\n",
    "\n",
    "# =======================\n",
    "#  Export to Parquet\n",
    "# =======================\n",
    "con.execute(\"\"\"\n",
    "COPY (SELECT * FROM imdb_final) \n",
    "TO 'imdb_merged_duckdb.parquet' (FORMAT PARQUET, COMPRESSION 'SNAPPY', ROW_GROUP_SIZE 100000);\n",
    "\"\"\")\n",
    "progress.update(1)\n",
    "progress.close()\n",
    "\n",
    "# =======================\n",
    "#  Clean up\n",
    "# =======================\n",
    "con.close()\n",
    "elapsed = (time.time() - start_time)\n",
    "print(f\"\\nâœ… Done! Merged dataset saved as imdb_merged_duckdb.parquet (Elapsed: {elapsed:.2f} seconds)\")\n",
    "print(\"   Reload it fast with: pd.read_parquet('imdb_merged_duckdb.parquet')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
